<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Talk Points</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #1a1a2e;
            color: #eee;
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
        }

        /* Search Area */
        .search-container {
            position: sticky;
            top: 0;
            background: #1a1a2e;
            padding: 15px 0 20px;
            z-index: 100;
        }

        #search {
            width: 100%;
            padding: 16px 24px;
            font-size: 20px;
            border: 2px solid #333;
            border-radius: 12px;
            background: #16213e;
            color: #fff;
            outline: none;
            transition: border-color 0.2s;
        }

        #search:focus {
            border-color: #4a9eff;
        }

        #search::placeholder {
            color: #666;
        }

        .hint {
            margin-top: 10px;
            font-size: 13px;
            color: #555;
        }

        .hint kbd {
            background: #333;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: inherit;
        }

        /* Topics List */
        .topics-list {
            margin-top: 10px;
        }

        .topic-card {
            background: #16213e;
            border-radius: 12px;
            padding: 24px 28px;
            margin-bottom: 16px;
            border-left: 4px solid #4a9eff;
            display: none;
            animation: fadeIn 0.15s ease;
        }

        .topic-card.visible {
            display: block;
        }

        .topic-card.highlighted {
            border-left-color: #00d4aa;
            background: #1a2744;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(-5px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .topic-title {
            font-size: 26px;
            font-weight: 600;
            color: #4a9eff;
            margin-bottom: 16px;
            display: flex;
            align-items: center;
            gap: 12px;
        }

        .topic-card.highlighted .topic-title {
            color: #00d4aa;
        }

        .topic-tags {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
            margin-bottom: 12px;
        }

        .tag {
            background: #2a3f5f;
            color: #8ab4f8;
            padding: 4px 10px;
            border-radius: 20px;
            font-size: 12px;
        }

        .points-list {
            list-style: none;
        }

        .points-list li {
            font-size: 22px;
            line-height: 1.5;
            padding: 10px 0;
            padding-left: 24px;
            position: relative;
            color: #ddd;
        }

        .points-list li::before {
            content: "‚Üí";
            position: absolute;
            left: 0;
            color: #4a9eff;
            font-weight: bold;
        }

        .topic-card.highlighted .points-list li::before {
            color: #00d4aa;
        }

        /* Shortcut Badge */
        .shortcut-badge {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 32px;
            height: 32px;
            background: linear-gradient(135deg, #00d4aa, #00b894);
            color: #1a1a2e;
            font-weight: 700;
            font-size: 16px;
            border-radius: 8px;
            margin-right: 12px;
            flex-shrink: 0;
        }

        .shortcut-key {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 24px;
            height: 24px;
            background: #2a3f5f;
            color: #00d4aa;
            font-weight: 700;
            font-size: 13px;
            border-radius: 6px;
            margin-right: 10px;
        }

        /* Architecture Diagram */
        .topic-diagram {
            margin: 16px 0 20px;
            text-align: center;
        }

        .topic-diagram img {
            max-width: 100%;
            max-height: 400px;
            border-radius: 12px;
            background: #fff;
            padding: 16px;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }

        .topic-diagram img:hover {
            transform: scale(1.02);
            box-shadow: 0 8px 32px rgba(0, 212, 170, 0.3);
        }

        /* Fullscreen overlay for diagram */
        .diagram-overlay {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.9);
            z-index: 1000;
            justify-content: center;
            align-items: center;
            cursor: pointer;
        }

        .diagram-overlay.visible {
            display: flex;
        }

        .diagram-overlay img {
            max-width: 95%;
            max-height: 95%;
            border-radius: 12px;
            background: #fff;
            padding: 20px;
        }

        /* Walkthrough Content */
        .walkthrough-content {
            margin-top: 24px;
            padding-top: 24px;
            border-top: 2px solid #2a3f5f;
        }

        .walkthrough-content h3 {
            color: #4a9eff;
            font-size: 22px;
            margin: 24px 0 12px;
            padding-bottom: 8px;
            border-bottom: 1px solid #2a3f5f;
        }

        .walkthrough-content h3:first-child {
            margin-top: 0;
        }

        .walkthrough-content h4 {
            color: #00d4aa;
            font-size: 18px;
            margin: 20px 0 10px;
        }

        .walkthrough-content p {
            color: #ccc;
            font-size: 18px;
            line-height: 1.6;
            margin: 12px 0;
        }

        .walkthrough-content strong {
            color: #fff;
        }

        .walkthrough-content ul,
        .walkthrough-content ol {
            margin: 12px 0 12px 24px;
            color: #ccc;
        }

        .walkthrough-content li {
            font-size: 17px;
            line-height: 1.6;
            margin: 8px 0;
        }

        .walkthrough-content pre {
            background: #0d1b2a;
            border: 1px solid #2a3f5f;
            border-radius: 8px;
            padding: 16px;
            font-family: 'Monaco', 'Menlo', monospace;
            font-size: 14px;
            color: #00d4aa;
            overflow-x: auto;
            margin: 16px 0;
        }

        .walkthrough-content code {
            background: #2a3f5f;
            color: #00d4aa;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Monaco', 'Menlo', monospace;
            font-size: 15px;
        }

        .walkthrough-content hr {
            border: none;
            border-top: 2px solid #2a3f5f;
            margin: 28px 0;
        }

        /* Counter */
        .counter {
            text-align: center;
            padding: 20px;
            color: #555;
            font-size: 14px;
        }

        /* Empty State */
        .empty-state {
            text-align: center;
            padding: 60px 20px;
            color: #555;
        }

        .empty-state h2 {
            font-size: 24px;
            margin-bottom: 10px;
            color: #666;
        }

        /* Edit Mode Button */
        .edit-toggle {
            position: fixed;
            bottom: 20px;
            right: 20px;
            background: #333;
            color: #888;
            border: none;
            padding: 10px 16px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 14px;
            transition: all 0.2s;
        }

        .edit-toggle:hover {
            background: #444;
            color: #fff;
        }

        /* All Topics View */
        .show-all .topic-card {
            display: block;
        }

        /* Index View */
        .index-view {
            display: none;
            flex-wrap: wrap;
            gap: 10px;
            padding: 20px 0;
        }

        .index-view.visible {
            display: flex;
        }

        .index-item {
            background: #16213e;
            padding: 12px 18px;
            border-radius: 8px;
            cursor: pointer;
            transition: all 0.15s;
            font-size: 15px;
        }

        .index-item:hover {
            background: #2a3f5f;
            transform: translateY(-2px);
        }

        /* Mode Toggle */
        .mode-toggle {
            display: flex;
            gap: 10px;
            margin-top: 12px;
        }

        .mode-btn {
            background: #16213e;
            border: 1px solid #333;
            color: #888;
            padding: 8px 16px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 13px;
            transition: all 0.2s;
        }

        .mode-btn:hover,
        .mode-btn.active {
            background: #2a3f5f;
            color: #fff;
            border-color: #4a9eff;
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="search-container">
            <input type="text" id="search" placeholder="Type to find topic... (e.g., 'cloud', 'team', 'metrics')"
                autocomplete="off" autofocus>
            <div class="hint">
                <kbd>A-Z</kbd> jump to topic &nbsp;
                <kbd>‚Üë‚Üì</kbd> navigate &nbsp;
                <kbd>Esc</kbd> clear &nbsp;
                <kbd>Tab</kbd> show all
            </div>
            <div class="mode-toggle">
                <button class="mode-btn active" data-mode="search">Search Mode</button>
                <button class="mode-btn" data-mode="index">Index View</button>
                <button class="mode-btn" data-mode="all">Show All</button>
            </div>
        </div>

        <div class="index-view" id="indexView"></div>

        <div class="topics-list" id="topicsList"></div>

        <div class="counter" id="counter"></div>
    </div>

    <!-- Fullscreen diagram overlay -->
    <div class="diagram-overlay" id="diagramOverlay" onclick="closeDiagram()">
        <img id="overlayImage" src="" alt="Architecture Diagram">
    </div>

    <button class="edit-toggle" onclick="openEditor()">‚úèÔ∏è Edit Topics</button>

    <script>
        // =====================================================
        // YOUR TOPICS DATA - EDIT THIS SECTION
        // =====================================================
        let topics = [
            {
                shortcut: "",
                title: "API Gateway",
                tags: ["routing", "rate-limiting", "authentication", "load-balancing"],
                points: [
                    "Single entry point for all microservices",
                    "Rate Limiting: Token bucket or sliding window algorithm",
                    "Authentication: JWT validation, OAuth 2.0 integration",
                    "Load Balancing: Round-robin, least connections, consistent hashing",
                    "Caching: Response caching for GET requests",
                    "Circuit Breaker: Prevent cascade failures",
                    "Examples: Kong, AWS API Gateway, Nginx"
                ]
            },
            {
                shortcut: "B",
                title: "Bitly - URL Shortener",
                image: "diagrams/bitly.png",
                tags: ["hashing", "database", "caching", "scale"],
                points: [],
                walkthrough: `
<h3>Phase 1: Requirements (2-3 mins)</h3>
<p><strong>What to say:</strong> "Let me clarify the requirements upfront"</p>

<p><strong>Functional (30 secs):</strong></p>
<ul>
    <li>Create short URL (with optional custom alias/expiration)</li>
    <li>Redirect short URL ‚Üí original URL</li>
</ul>

<p><strong>Non-functional (1.5 mins):</strong></p>
<ul>
    <li>Scale: 1B URLs, 100M DAU</li>
    <li>Read:Write ratio 1000:1 (critical for architecture decisions)</li>
    <li>Latency: <100ms redirect</li>
    <li>Availability: 99.99%</li>
</ul>

<p><strong>Stop here:</strong> Don't list every possible feature. Focus on the 3-4 that drive design decisions.</p>

<hr>

<h3>Phase 2: Core Entities & API (1-2 mins)</h3>
<p><strong>Quick whiteboard:</strong></p>
<pre>Entities: URL(short_code, original_url, created_at, expires_at)

API:
POST /urls ‚Üí {short_url}
GET /{code} ‚Üí 302 redirect</pre>

<p><strong>Why this works:</strong> You've established the contract. Don't dwell on every field‚Äîmention you'll detail the data model during deep dives.</p>

<hr>

<h3>Phase 3: High-Level Architecture (2-3 mins)</h3>
<p><strong>Draw the flow:</strong></p>
<pre>                        Client
                          ‚Üì
                    Load Balancer
                     ‚Üì          ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   Write    ‚îÇ ‚îÇ    Read    ‚îÇ
              ‚îÇ  Service   ‚îÇ ‚îÇ  Service   ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì           ‚Üì     ‚Üì
             Global Counter   Cache  Database
             (ID generation) (Redis) (Postgres)
                    ‚Üì                  ‚Üë
                Database ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

  POST /shorten ‚Üí Write path       GET /:shortCode ‚Üí Read path</pre>

<p><strong>Key callouts (30 secs each):</strong></p>
<ol>
    <li><strong>Separation of concerns:</strong> Write Service handles creation, Read Service handles redirects</li>
    <li><strong>Caching strategy:</strong> Redis for hot URLs (addresses 1000:1 read ratio)</li>
    <li><strong>Database choice:</strong> Postgres/MySQL sufficient for 1B URLs (~500GB)</li>
</ol>

<p><strong>Don't:</strong> Spend time on load balancer mechanics or basic patterns. Save time for deep dives.</p>

<hr>

<h3>Phase 4: Critical Deep Dives (6-8 mins)</h3>
<p><strong>This is where you differentiate yourself.</strong> Pick 2-3 based on interviewer interest:</p>

<h4>Deep Dive A: Short Code Generation (2-3 mins)</h4>
<p><strong>The problem:</strong> Need globally unique, compact codes</p>
<p><strong>Solution:</strong> Base62 encoding with global counter</p>
<ul>
    <li>1B URLs = 6 characters (62^6 = 56B)</li>
    <li>Counter stored in Redis (atomic increment)</li>
    <li>Write Service: <code>INCR counter ‚Üí Base62(value) ‚Üí store mapping</code></li>
</ul>

<p><strong>Trade-off mention:</strong> "Hash-based approaches (MD5/SHA) are simpler but risk collisions and require longer codes. Counter guarantees uniqueness but needs distributed consensus."</p>

<p><strong>Advanced point (if time):</strong> Counter batching to reduce Redis calls‚Äîeach Write Service instance gets a batch of 1000 values</p>

<h4>Deep Dive B: Fast Redirects (<100ms) (2-3 mins)</h4>
<p><strong>The problem:</strong> Billions of URLs, can't do full table scans</p>

<p><strong>Layered approach:</strong></p>
<ol>
    <li><strong>L1 - In-memory cache (Redis):</strong>
        <ul>
            <li>Cache hot URLs (80/20 rule)</li>
            <li>TTL matches URL expiration</li>
            <li>Hit rate: 95%+</li>
        </ul>
    </li>
    <li><strong>L2 - Database index:</strong>
        <ul>
            <li>B-tree index on <code>short_code</code> column</li>
            <li>O(log n) lookups</li>
        </ul>
    </li>
    <li><strong>L3 - CDN/Edge (bonus):</strong>
        <ul>
            <li>302 redirects at edge locations</li>
            <li>Reduces latency for popular links</li>
        </ul>
    </li>
</ol>

<p><strong>Key metric:</strong> "With Redis hit rate >95%, most redirects never touch the database"</p>

<h4>Deep Dive C: Scaling to 1B URLs (2 mins)</h4>
<p><strong>Horizontal scaling strategy:</strong></p>

<p><strong>Read Service:</strong></p>
<ul>
    <li>Stateless‚Äîadd more instances behind load balancer</li>
    <li>Redis cluster for cache distribution</li>
    <li>Database read replicas (read-heavy workload)</li>
</ul>

<p><strong>Write Service:</strong></p>
<ul>
    <li>Challenge: Global counter needs coordination</li>
    <li>Solution: Redis single-threaded = natural serialization</li>
    <li>With batching: 100k writes/sec easily achievable</li>
</ul>

<p><strong>Database sharding (mention, don't elaborate):</strong></p>
<ul>
    <li>"At 3.5 trillion URLs we'd shard by short_code range"</li>
    <li>"But 500GB easily fits on modern Postgres‚Äîno need yet"</li>
</ul>

<hr>

<h3>Phase 5: Wrap-Up (1-2 mins)</h3>
<p><strong>Proactively mention trade-offs:</strong></p>

<p><strong>What we optimized for:</strong></p>
<ul>
    <li>Read performance (caching, indexing, 302 redirects)</li>
    <li>Uniqueness guarantees (counter-based generation)</li>
    <li>Simplicity (Postgres over distributed DB)</li>
</ul>

<p><strong>What we sacrificed:</strong></p>
<ul>
    <li>Counter is single point of failure ‚Üí solved with Redis replication</li>
    <li>302 means every click goes through our system ‚Üí acceptable for analytics</li>
    <li>No immediate consistency for distributed writes ‚Üí fine for this use case</li>
</ul>

<p><strong>Areas we didn't cover (offer to discuss):</strong></p>
<ul>
    <li>Analytics pipeline</li>
    <li>Abuse prevention (rate limiting, spam detection)</li>
    <li>Custom domain support</li>
</ul>
`
            },
            {
                shortcut: "C",
                title: "Chat System (Slack)",
                tags: ["websocket", "channels", "presence", "notifications"],
                points: [
                    "Real-time: WebSocket connections per user session",
                    "Channels: Pub/Sub model with Redis or dedicated message broker",
                    "Message Storage: Append-only log, partition by channel",
                    "Presence: Heartbeat + distributed cache for online status",
                    "Search: Elasticsearch for message history",
                    "Notifications: Push via FCM/APNs for mobile",
                    "File Sharing: CDN for attachments, pre-signed URLs"
                ]
            },
            {
                shortcut: "D",
                title: "DRIFT Infra ‚Äî Event Streaming Platform",
                tags: ["Kafka", "Flink", "streaming", "platform", "self-service", "reliability"],
                points: [
                    "Context: Own Kafka + Flink event platform ‚Äî ~2B events/day, 300+ producers",
                    "Powers tech ref data, regulatory reporting, risk calculations + ops telemetry",
                    "Segmented clusters so telemetry can't impact business streams",
                    "Problem: Bespoke onboarding (~150 producers) ‚Üí platform team bottleneck, over-provisioned cost",
                    "Actions: Producer experience first (SDK/portal, Avro contracts, DLQ), then efficiency (Flink autoscaling, tiered storage)",
                    "Results: 1B‚Üí2B/day, producers 150‚Üí300+, cost -30%, 99.9%+ reliability for 18+ months",
                    "Learning: Scale from removing humans as bottlenecks; sequence leverage first, then optimize cost"
                ],
                walkthrough: `
<h3>DRIFT Infra</h3>

<h4>Context</h4>
<ul>
    <li>Own Kafka + Flink event platform: <strong>~2B events/day, 300+ producers</strong></li>
    <li>Powers tech ref data, regulatory reporting, risk calculations + ops telemetry
        <ul>
            <li>Container metrics + vulnerabilities ‚Äî CVM, VTM</li>
            <li>Incidents, change records</li>
            <li>100's of different related categories</li>
        </ul>
    </li>
    <li><strong>Segmented clusters</strong> so telemetry can't impact business streams</li>
    <li>If it lags/fails ‚Üí <strong>EOD + compliance cascades</strong></li>
</ul>

<h4>Starting Problems + Mandate</h4>
<ul>
    <li><strong>Bespoke onboarding</strong> (~150 producers) ‚Üí platform team bottleneck</li>
    <li><strong>Over-provisioned cost</strong></li>
    <li>Mandate: <strong>self-service scale, reduce cost, +reliability, headcount flat</strong></li>
</ul>

<h4>Actions (Sequenced) ‚Üí Organizational Architecture, Not Just Technical</h4>

<h4>1. Producer Experience First</h4>
<ul>
    <li><strong>Standard contracts:</strong> Avro + registry; <strong>edge validation + DLQ/quarantine</strong></li>
    <li><strong>Golden path onboarding:</strong> SDK/portal + <strong>prod readiness gates</strong></li>
    <li><strong>Adoption:</strong> pilot with Risk + Ops, workshops, <strong>6-month deprecation</strong> w/ BU exec alignment</li>
</ul>

<h4>2. Efficiency Second</h4>
<ul>
    <li><strong>Flink autoscaling</strong>, compression, <strong>tiered storage</strong></li>
    <li>Managed tradeoff: tier latency ‚Üí <strong>caching + tiered SLAs</strong></li>
</ul>

<h4>Results</h4>
<ul>
    <li>Throughput: <strong>1B ‚Üí 2B/day</strong> (no infra add)</li>
    <li>Producers: <strong>150 ‚Üí 300+</strong>, onboarding <strong>weeks ‚Üí hours</strong></li>
    <li>Cost: <strong>-30%</strong>; savings funded resilience work</li>
    <li>Headcount: <strong>flat</strong>; team shifted from toil ‚Üí platform engineering</li>
    <li>Reliability: <strong>99.9%+ for 18+ months</strong></li>
</ul>

<h4>Key Learnings</h4>
<ul>
    <li>Scale comes from <strong>removing humans as bottlenecks</strong></li>
    <li>Resistance was <strong>cultural</strong> ‚Üí pilots + exec-backed deprecation beat mandates</li>
    <li><strong>Sequence leverage first</strong>, then optimize cost/perf</li>
</ul>

<hr>

<h4>Tooling Reference (For Recall)</h4>
<table>
    <tr><th>Tool</th><th>What It Is</th><th>Main Use / Function</th></tr>
    <tr><td><strong>dbt</strong> (data build tool)</td><td>Data transformation framework</td><td>Transforms and models data <em>inside the data warehouse</em> using SQL</td></tr>
    <tr><td><strong>Fivetran</strong></td><td>Managed data ingestion (EL) platform</td><td>Automatically extracts data from APIs & databases and loads into a data warehouse</td></tr>
    <tr><td><strong>Airbyte</strong></td><td>Open-source data ingestion (EL/ELT) platform</td><td>Moves data from sources to destinations like Fivetran does</td></tr>
    <tr><td><strong>Dagster</strong></td><td>Modern data orchestration framework</td><td>Defines, schedules, and monitors data pipelines in Python</td></tr>
    <tr><td><strong>Apache Airflow</strong></td><td>Workflow orchestration platform</td><td>Schedules and manages complex data workflows (ETL, scripts, dbt jobs)</td></tr>
</table>

<hr>

<h4>Sketch If Asked</h4>
<pre>[Producers (300+)]
        ‚Üì
[API Gateway] ‚Üê (on-prem systems connect here)
        ‚Üì
[Schema Registry] ‚Üí validation
        ‚Üì
[Kafka/MSK]
        ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
    ‚Üì       ‚Üì
[Flink]  [Real-time Consumers]
    ‚Üì
    ‚îú‚îÄ‚îÄ‚Üí [S3 ‚Äî Raw/Parquet]
    ‚îú‚îÄ‚îÄ‚Üí [Snowflake ‚Äî Analytics]
    ‚îî‚îÄ‚îÄ‚Üí [Operational DBs]</pre>

<hr>

<h4>If They Probe Deeper</h4>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:30%;">Probe</th><th style="border:1px solid #555; padding:8px; text-align:left;">Your Answer</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;">"How do you handle hot partitions?"</td><td style="border:1px solid #555; padding:8px;">"Partition by producer ID typically. For high-volume producers, we spread across multiple partitions. Flink aggregates regardless of partition distribution."</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;">"What if Kafka goes down?"</td><td style="border:1px solid #555; padding:8px;">"Multi-AZ, RF-3. Survive AZ failure with no data loss. Producers buffer locally and retry ‚Äî SDKs have backoff built in."</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;">"How do you handle schema evolution?"</td><td style="border:1px solid #555; padding:8px;">"Glue Schema Registry enforces backward compatibility. Breaking changes rejected at registration. Teams can add fields, not remove or change types."</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;">"What about exactly-once?"</td><td style="border:1px solid #555; padding:8px;">"Kafka supports idempotent producers. Flink checkpoints offsets with state. For critical paths, we verify with batch reconciliation."</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;">"How do consumers handle backpressure?"</td><td style="border:1px solid #555; padding:8px;">"Consumer lag monitoring. If a consumer falls behind, we alert. It's their capacity problem ‚Äî platform provides the durability."</td></tr>
</table>
`
            },
            {
                shortcut: "E",
                title: "E-Commerce (Amazon)",
                tags: ["catalog", "cart", "inventory", "checkout"],
                points: [
                    "Catalog Service: Product data with Elasticsearch search",
                    "Inventory: Real-time stock with Redis, eventual consistency",
                    "Cart: Session-based (Redis) or persistent (database)",
                    "Checkout: Saga pattern for distributed transactions",
                    "Recommendations: Collaborative filtering, ML models",
                    "CDN: Product images, static assets globally distributed",
                    "Payment: Idempotency keys, retry with backoff"
                ]
            },
            {
                shortcut: "F",
                title: "Facebook News Feed",
                tags: ["fanout", "ranking", "caching", "timeline"],
                points: [
                    "Push vs Pull: Fanout-on-write vs fanout-on-read",
                    "Hybrid: Push for normal users, pull for celebrities",
                    "Feed Storage: Redis sorted sets per user (post_id, timestamp)",
                    "Ranking: ML model scores posts (engagement, recency, affinity)",
                    "Graph DB: Neo4j or TAO for social connections",
                    "Caching: User feed cached, invalidate on new post",
                    "Pagination: Cursor-based (not offset) for consistency"
                ],
                walkthrough: `
<h3>News Aggregator ‚Äî System Design Walkthrough</h3>

<h4>Functional Requirements</h4>
<ul>
    <li>Users view an <strong>aggregated feed</strong> of news articles from thousands of source publishers worldwide</li>
    <li>Users can <strong>scroll infinitely</strong> through the feed</li>
    <li>Users click articles ‚Üí <strong>redirected to publisher's website</strong> for full content</li>
</ul>

<h4>Non-Functional Requirements</h4>
<ul>
    <li><strong>Availability over consistency</strong> (CAP theorem) ‚Äî users prefer slightly outdated content over no content</li>
    <li>Scalable to <strong>100M DAU</strong>, spikes up to 500M</li>
    <li>Low latency feed load times: <strong>< 200ms</strong></li>
</ul>

<h4>Core Entities</h4>
<ul>
    <li><strong>Article:</strong> id, title, summary, thumbnailUrl, publishDate, url, publisherId, region, mediaUrls</li>
    <li><strong>Publisher:</strong> id, name, url, rssFeedUrl, region, lastScraped</li>
    <li><strong>User:</strong> id, region (inferred from IP or explicit)</li>
</ul>

<h4>API / System Interface</h4>
<ul>
    <li><code>GET /feed?page={page}&limit={limit}&region={region} ‚Üí Article[]</code></li>
    <li>Start with offset-based pagination ‚Üí improve to <strong>cursor-based</strong> for scale + infinite scrolling</li>
    <li>No endpoint for single article ‚Äî browser redirects to publisher's <code>url</code> field</li>
</ul>

<h4>High-Level Design</h4>

<h4>1. Data Collection (Ingestion)</h4>
<ul>
    <li><strong>Data Collection Service:</strong> polls publisher RSS feeds and APIs every 3-6 hours based on update frequency</li>
    <li>Queries DB for publisher list + RSS URLs ‚Üí fetches new articles ‚Üí stores in DB</li>
    <li><strong>Thumbnails:</strong> downloaded and stored in S3/Object Storage (don't rely on publisher servers ‚Äî may be slow/down)</li>
    <li>Standardize image quality/size for consistent UX</li>
</ul>

<h4>2. Feed Service (Serving)</h4>
<ul>
    <li><strong>Client</strong> ‚Üí API Gateway ‚Üí <strong>Feed Service</strong> ‚Üí Database</li>
    <li>Feed Service queries relevant articles based on user's <strong>region</strong> and formats response</li>
    <li>API Gateway handles auth, rate limiting, request validation</li>
</ul>

<h4>Architecture Flow</h4>
<pre>  [External Publishers] ‚Üê‚îÄ RSS ‚îÄ‚Üí [Data Collection Service] ‚îÄ‚îÄ‚Üí [Database]
                                            ‚îÇ                       ‚Üë
                                            ‚Üì                       ‚îÇ
                                         [S3]                  [Feed Service]
                                    (thumbnails)                    ‚Üë
                                                                    ‚îÇ
                                                    [API Gateway] ‚Üê‚îÄ‚îÄ [Client]</pre>

<hr>

<h3>Infinite Scrolling ‚Äî How It Works</h3>
<ul>
    <li><strong>Initial load:</strong> Client sends <code>GET /feed?region=US&limit=20&page=1</code></li>
    <li>Feed Service queries first 20 articles ordered by publish date</li>
    <li>Response includes articles + pagination metadata (<code>total_pages</code>, <code>current_page</code>)</li>
    <li><strong>As user scrolls:</strong> Client auto-sends next page request (<code>page=2</code>)</li>
    <li>Feed Service calculates offset: <code>(page-1) * limit</code>, fetches next batch</li>
    <li>DB query uses <code>OFFSET</code> and <code>LIMIT</code> clauses</li>
</ul>

<h3>Article Click ‚Üí Redirect</h3>
<ul>
    <li>No backend needed ‚Äî browser redirects to article's <code>url</code> field stored in DB</li>
    <li>News aggregators <strong>don't host content</strong> ‚Äî they point to the publisher's website</li>
</ul>

<hr>

<h3>Deep Dives</h3>

<h4>1. Cursor-Based Pagination (Replace Offset)</h4>
<ul>
    <li><strong>Problem:</strong> Offset-based pagination breaks when new articles are constantly published ‚Äî user on page 2 sees duplicates or misses articles as content shifts</li>
    <li>50-100 new articles/hour during peak ‚Üí 10 min browsing = pagination drift guaranteed</li>
    <li><strong>Solution: Monotonically increasing Article IDs</strong> (ULIDs or auto-increment)</li>
    <li>Newer articles always have higher IDs ‚Üí use article ID as cursor</li>
    <li>Query: <code>WHERE article_id < cursor_id ORDER BY article_id DESC LIMIT 20</code></li>
    <li>No composite cursors, no timestamp handling ‚Äî just a single ID value</li>
    <li>Simple index on <code>article_id</code> column ‚Äî fast and simple</li>
    <li><strong>Challenge:</strong> Requires planning ID strategy upfront; migrating from random UUIDs later is painful</li>
    <li>For distributed systems: use ULID generation or centralized ID services to maintain ordering</li>
</ul>

<h4>2. Low Latency Feed (< 200ms)</h4>
<ul>
    <li><strong>Problem:</strong> Querying DB directly for every feed request won't scale ‚Äî 100M DAU √ó 5-10 refreshes = 500M-1B requests/day</li>
    <li>Querying millions of articles + filtering by region ‚Üí response times well beyond 200ms</li>
    <li><strong>Solution: Redis Read Cache</strong></li>
    <li>Pre-compute regional feeds ‚Üí store in Redis sorted sets</li>
    <li>Feed Service checks cache first ‚Üí DB fallback only on cache miss</li>
    <li><strong>Cache invalidation:</strong> When Data Collection Service ingests new articles ‚Üí update regional cache</li>
    <li>Redis latency: <strong>sub-millisecond</strong> vs DB: 10-50ms+</li>
</ul>

<h4>3. Media Content ‚Äî Thumbnails at Scale</h4>
<ul>
    <li><strong>Problem:</strong> 100M+ users viewing feeds ‚Äî thumbnail delivery must be fast and cost-effective</li>
    <li>Publisher images may be slow, change URLs, or go down ‚Äî need our own copies</li>
    <li><strong>Bad:</strong> Database blob storage (expensive, slow, doesn't scale)</li>
    <li><strong>Good:</strong> S3 with direct links (scalable but no global edge caching)</li>
    <li><strong>Best: S3 + CloudFront CDN with multiple sizes</strong></li>
    <li>Generate multiple thumbnail sizes: <strong>150√ó100</strong> (mobile), <strong>300√ó200</strong> (desktop), <strong>600√ó400</strong> (retina)</li>
    <li>Use HTML <code>srcset</code> or client-side logic to request appropriate version</li>
    <li>CloudFront serves from 400+ edge locations ‚Üí sub-10ms globally</li>
</ul>

<h4>Final Architecture Flow</h4>
<pre>  [Client] ‚Üí [API Gateway] ‚Üí [Feed Service] ‚Üí [Redis Cache] ‚Üí [Database]
       ‚Üì              ‚Üì                                              ‚Üë
   [CDN] ‚Üê‚îÄ‚îÄ [S3]    [Webhook Service]              [Data Collection Services]
  (thumbnails)                                     (High/Med/Low Priority)
                                                           ‚Üì
                                                   [External Publishers]
                                                   (RSS Feed / Web Scraper)</pre>
`
            },
            {
                shortcut: "G",
                title: "Golden Path ‚Äî Platform Governance",
                tags: ["platform", "Terraform", "OPA", "self-service", "governance", "security"],
                points: [
                    "Context: Took over infra + platform teams ‚Äî devs blocked by tickets for IAM, networking, pipeline setup",
                    "Problem: Security gate-keeping standard requests, days/weeks in approval queues ‚Äî system issue, not tooling",
                    "Action: Pitched shared win (Security + Engineering + Leadership), built Terraform + OPA policy-as-code",
                    "Structured: Core Platform team + Developer Experience team, partnered with CISO",
                    "Result: 75% adoption in 4 months, same-day deploys, 60% drop in IAM/network misconfigs, 40% scale increase",
                    "Learning: Automate governance, align incentives, discipline in keeping starting point simple"
                ],
                walkthrough: `
<h3>Golden Path</h3>

<h4>Context</h4>
<ul>
    <li>Took over the infra and platform teams</li>
    <li>Inherited a standoff: devs blocked by tickets</li>
    <li>Every new service takes days sometimes weeks in approval queues
        <ul>
            <li>IAM Roles</li>
            <li>Networking configs</li>
            <li>Pipeline setup</li>
        </ul>
    </li>
    <li>Security was gate keeping and manually reviewing standard requests</li>
    <li>Developers were frustrated</li>
    <li>I saw a system issue not a tooling problem</li>
    <li>The process was clearly broken</li>
</ul>

<h4>Action</h4>
<ul>
    <li>Buy in first ‚Äî pitched shared win to Security + Engineering + Leadership</li>
    <li>Security gets automated enforcement they can trust</li>
    <li>Engineering gets self service speed with no ticket queues</li>
    <li>Structured work across two managers:
        <ul>
            <li>Core Platform team</li>
            <li>Developer Experience team</li>
        </ul>
    </li>
    <li>Partnered directly with security architecture (CISO)</li>
    <li>Two capabilities:
        <ul>
            <li>Standardized provisioning: (Terraform) ‚Üí 80% of common workloads</li>
            <li>Policy-as-code (OPA) ‚Äî Security checks in pipeline, enforced before merge</li>
        </ul>
    </li>
    <li>Deliberately scoped to the 80% and pushed back on edge cases</li>
    <li>I ensured alignment and stayed opinionated</li>
</ul>

<h4>Result</h4>
<ul>
    <li><strong>75% adoption</strong> in four months post implementation without mandates ‚Äî everyone loved it</li>
    <li>Weeks ‚Üí <strong>same-day</strong> time to first production deploy</li>
    <li><strong>60% drop</strong> in IAM & Network misconfigurations (no longer subjective ‚Äî declarative policy based guard rail)</li>
    <li>Platform scale increased by <strong>40%</strong> without adding headcount</li>
</ul>

<h4>Learning</h4>
<ul>
    <li>Governance and provisioning needs to be as automated as possible</li>
    <li>Once incentives align, the technical work is straightforward</li>
    <li>Discipline in keeping starting point simple</li>
</ul>
`
            },
            {
                shortcut: "H",
                title: "Hotel Booking (Airbnb)",
                tags: ["search", "booking", "availability", "pricing"],
                points: [
                    "Search: Elasticsearch with geo-filters, date availability",
                    "Availability: Calendar service with date range queries",
                    "Booking: Pessimistic locking during checkout flow",
                    "Pricing: Dynamic pricing based on demand, seasonality",
                    "Reviews: Eventually consistent, aggregate ratings cached",
                    "Messaging: In-app chat between host and guest",
                    "Payments: Escrow until check-in confirmed"
                ]
            },
            {
                shortcut: "I",
                title: "Intro ‚Äî Self Introduction",
                tags: ["intro", "Citi", "Mastercard", "platform", "leadership"],
                points: [
                    "I'm Sogo ‚Äî Senior Engineering Manager at Citi",
                    "25-person team: 3 managers + 2 architects, CTS organisation",
                    "My team runs platform infrastructure powering our data platform",
                    "Also help with cloud migrations",
                    "Before Citi: Blockchain infrastructure at Mastercard ‚Äî L2 rollup migration"
                ],
                walkthrough: `
<h3>Introduction</h3>
<ul>
    <li>I'm <strong>Sogo</strong> ‚Äî Senior Engineering Manager at Citi</li>
    <li><strong>25-person team:</strong> 3 managers + 2 architects, CTS organisation</li>
    <li>My team runs <strong>platform infrastructure</strong> powering our data platform</li>
    <li>Also help with <strong>cloud migrations</strong></li>
    <li>Before Citi: <strong>Blockchain infrastructure at Mastercard</strong> ‚Äî L2 rollup migration</li>
</ul>

<hr>

<h4>High-Performing Teams ‚Äî Project Aristotle Summary</h4>
<p><strong>The Five Factors (In Order of Impact)</strong></p>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:30%;">Factor</th><th style="border:1px solid #555; padding:8px; text-align:left;">What It Means</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>1. Psychological Safety</strong></td><td style="border:1px solid #555; padding:8px;">Safe to take risks, ask questions, admit mistakes ‚Äî without fear</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>2. Dependability</strong></td><td style="border:1px solid #555; padding:8px;">People deliver what they commit to</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>3. Structure & Clarity</strong></td><td style="border:1px solid #555; padding:8px;">Clear roles, goals, expectations</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>4. Meaning</strong></td><td style="border:1px solid #555; padding:8px;">The work matters personally</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>5. Impact</strong></td><td style="border:1px solid #555; padding:8px;">The team believes their work makes a difference</td></tr>
</table>

<hr>

<h4>Question 1: Leading a Team with Juniors and Non-Coders in SRE</h4>

<p><strong>Director-Level Answer</strong></p>
<blockquote>
"First, I'd understand what each person brings. Non-coders in SRE often have deep operational knowledge ‚Äî they know the systems, the failure modes, the tribal knowledge. That's valuable. Juniors bring energy and fresh perspective. My job is building a team where different strengths complement each other."
</blockquote>

<p><strong>Three Things I'd Focus On</strong></p>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:25%;">Focus</th><th style="border:1px solid #555; padding:8px; text-align:left;">How</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Pair expertise</strong></td><td style="border:1px solid #555; padding:8px;">Non-coders partner with coders on automation projects ‚Äî domain knowledge meets implementation skill</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Create learning paths</strong></td><td style="border:1px solid #555; padding:8px;">Juniors get mentorship and stretch assignments. Non-coders get supported path to scripting/automation if they want it ‚Äî but it's optional, not forced</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Value all contributions</strong></td><td style="border:1px solid #555; padding:8px;">Runbooks, incident response, stakeholder communication ‚Äî not everything is code. Recognize operational excellence, not just commits</td></tr>
</table>

<hr>

<h4>Question 2: What is Quality Software?</h4>

<p><strong>Director-Level Answer</strong></p>
<blockquote>
"Quality software does what it's supposed to do, reliably, and is easy to change when requirements evolve."
</blockquote>

<p><strong>Three Dimensions</strong></p>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:25%;">Dimension</th><th style="border:1px solid #555; padding:8px; text-align:left;">What It Means</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Correctness</strong></td><td style="border:1px solid #555; padding:8px;">Does what users expect ‚Äî meets functional requirements</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Reliability</strong></td><td style="border:1px solid #555; padding:8px;">Works consistently ‚Äî handles failures gracefully, doesn't surprise you at 2am</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Maintainability</strong></td><td style="border:1px solid #555; padding:8px;">Easy to understand, test, and change ‚Äî future developers can work with it</td></tr>
</table>

<p><strong>What I'd Add for SRE Context</strong></p>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:25%;">Quality</th><th style="border:1px solid #555; padding:8px; text-align:left;">Why It Matters</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Observable</strong></td><td style="border:1px solid #555; padding:8px;">You can see what it's doing ‚Äî logs, metrics, traces. No black boxes.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Operable</strong></td><td style="border:1px solid #555; padding:8px;">Easy to deploy, rollback, scale. Doesn't require heroics to run.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Resilient</strong></td><td style="border:1px solid #555; padding:8px;">Fails gracefully. Degrades, doesn't collapse.</td></tr>
</table>

<hr>

<h2>AWS Infrastructure Knowledge Reference</h2>
<p><em>What a hands-on Director of Infrastructure must know cold</em></p>

<h3>Regions, Availability Zones & Edge</h3>
<p>Understanding the physical topology of AWS is foundational. Every architecture decision ‚Äî latency, redundancy, compliance, cost ‚Äî traces back to how you use regions and AZs.</p>

<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:15%;">Concept</th><th style="border:1px solid #555; padding:8px; text-align:left; width:25%;">What It Is</th><th style="border:1px solid #555; padding:8px; text-align:left; width:30%;">Key Numbers</th><th style="border:1px solid #555; padding:8px; text-align:left;">Interview Relevance</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Region</strong></td><td style="border:1px solid #555; padding:8px;">Geographically isolated cluster of data centers (e.g., eu-west-1 Dublin). Each region is fully independent.</td><td style="border:1px solid #555; padding:8px;">30+ regions globally. Inter-region latency: 50-200ms depending on distance.</td><td style="border:1px solid #555; padding:8px;">Choose region for: data residency/compliance, proximity to users, service availability, cost differences.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Availability Zone (AZ)</strong></td><td style="border:1px solid #555; padding:8px;">One or more discrete data centers within a region, with independent power, cooling, networking. Connected via low-latency private fiber.</td><td style="border:1px solid #555; padding:8px;">Typically 3 AZs per region (some have 6). Intra-AZ latency: < 1ms. Cross-AZ latency: 1-2ms. Cross-AZ data transfer: ~$0.01/GB.</td><td style="border:1px solid #555; padding:8px;">Multi-AZ = high availability. Single AZ = lower cost/latency. Always deploy stateless services across 2+ AZs.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Local Zones</strong></td><td style="border:1px solid #555; padding:8px;">AWS infrastructure placed in cities close to end users. Extension of a parent region.</td><td style="border:1px solid #555; padding:8px;">30+ Local Zones. Single-digit ms latency to nearby users.</td><td style="border:1px solid #555; padding:8px;">Use for: real-time gaming, video streaming, ML inference at the edge. NOT for core backend services.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Edge Locations</strong></td><td style="border:1px solid #555; padding:8px;">CloudFront CDN and Route 53 DNS endpoints. Serve cached content and DNS from 400+ locations worldwide.</td><td style="border:1px solid #555; padding:8px;">400+ edge locations. Sub-10ms DNS resolution.</td><td style="border:1px solid #555; padding:8px;">Static content, API caching, DDoS protection (Shield). Always mention CloudFront for read-heavy, globally distributed workloads.</td></tr>
</table>

<blockquote>
üí° <em>When asked 'how do you handle multi-region?' the answer depends on WHY: latency (CloudFront/Local Zones), compliance (data residency), or disaster recovery (active-passive vs active-active). Always clarify the driver before proposing architecture.</em>
</blockquote>

<h3>AZ Design Patterns</h3>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:20%;">Pattern</th><th style="border:1px solid #555; padding:8px; text-align:left; width:40%;">How It Works</th><th style="border:1px solid #555; padding:8px; text-align:left;">When To Use</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Multi-AZ Active-Active</strong></td><td style="border:1px solid #555; padding:8px;">Services run in 2-3 AZs behind a load balancer. All AZs serve traffic simultaneously. ALB/NLB routes across AZs automatically.</td><td style="border:1px solid #555; padding:8px;">Default for all production stateless services. ECS/EKS tasks spread across AZs.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Multi-AZ Active-Standby</strong></td><td style="border:1px solid #555; padding:8px;">Primary in one AZ, standby in another. Automatic failover on failure. RDS Multi-AZ does this natively (synchronous replication, ~60s failover).</td><td style="border:1px solid #555; padding:8px;">Databases (RDS Multi-AZ), stateful workloads where active-active adds complexity.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Single-AZ</strong></td><td style="border:1px solid #555; padding:8px;">Everything in one AZ. No cross-AZ data transfer costs. No redundancy.</td><td style="border:1px solid #555; padding:8px;">Dev/test environments, cost-sensitive batch processing, workloads where downtime is acceptable.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Multi-Region Active-Passive</strong></td><td style="border:1px solid #555; padding:8px;">Primary region serves all traffic. Secondary region has infrastructure on standby. DNS failover (Route 53) switches traffic on primary failure.</td><td style="border:1px solid #555; padding:8px;">Disaster recovery. RPO depends on replication lag (Aurora Global: ~1s). RTO depends on failover automation (minutes to hours).</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Multi-Region Active-Active</strong></td><td style="border:1px solid #555; padding:8px;">Both regions serve traffic. DynamoDB Global Tables or Aurora Global Database for data sync. Route 53 latency-based routing.</td><td style="border:1px solid #555; padding:8px;">Global user base needing < 100ms latency everywhere. Very complex ‚Äî conflict resolution required.</td></tr>
</table>

<hr>

<h2>VPC ‚Äî Virtual Private Cloud</h2>
<p>VPC is the network foundation of everything you build on AWS. If you can't trace how a packet gets from the internet to your container and back, nothing else matters.</p>

<h3>VPC Building Blocks</h3>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:12%;">Component</th><th style="border:1px solid #555; padding:8px; text-align:left; width:25%;">What It Is</th><th style="border:1px solid #555; padding:8px; text-align:left; width:30%;">Key Details</th><th style="border:1px solid #555; padding:8px; text-align:left;">Common Mistakes</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>VPC</strong></td><td style="border:1px solid #555; padding:8px;">Your isolated virtual network. A logical boundary for all your resources in a region.</td><td style="border:1px solid #555; padding:8px;">CIDR block (e.g., 10.0.0.0/16 = 65,536 IPs). Spans all AZs in a region. Cannot change CIDR after creation (but can add secondary CIDRs).</td><td style="border:1px solid #555; padding:8px;">Making CIDR too small. Use /16 for production VPCs ‚Äî IP exhaustion is painful to fix later.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Subnet</strong></td><td style="border:1px solid #555; padding:8px;">A range of IPs within a VPC, tied to a single AZ. Where resources actually live.</td><td style="border:1px solid #555; padding:8px;">Public subnet: has route to Internet Gateway. Private subnet: no direct internet route. Typically 3 public + 3 private (one per AZ).</td><td style="border:1px solid #555; padding:8px;">Putting databases in public subnets. Only load balancers and bastion hosts belong in public subnets.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Internet Gateway (IGW)</strong></td><td style="border:1px solid #555; padding:8px;">Connects VPC to the internet. Allows resources with public IPs to be reached from outside.</td><td style="border:1px solid #555; padding:8px;">Highly available by design (AWS managed). One per VPC. Free to use.</td><td style="border:1px solid #555; padding:8px;">Forgetting that IGW alone isn't enough ‚Äî subnet route table must have 0.0.0.0/0 pointing to it.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>NAT Gateway</strong></td><td style="border:1px solid #555; padding:8px;">Allows private subnet resources to reach the internet (for patches, API calls) without being reachable FROM the internet.</td><td style="border:1px solid #555; padding:8px;">~$0.045/hour + $0.045/GB processed. Deploy one per AZ for HA. Up to 45 Gbps bandwidth.</td><td style="border:1px solid #555; padding:8px;">Single NAT Gateway = single AZ failure point. Cost surprise at high volume ‚Äî 2B events/day at 1KB = ~$2.7K/month in NAT charges alone.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Route Table</strong></td><td style="border:1px solid #555; padding:8px;">Rules that determine where network traffic is directed. Each subnet is associated with one route table.</td><td style="border:1px solid #555; padding:8px;">Local route (VPC CIDR) is always present and cannot be removed. Add 0.0.0.0/0 ‚Üí IGW for public, 0.0.0.0/0 ‚Üí NAT for private.</td><td style="border:1px solid #555; padding:8px;">Forgetting to associate the route table with the subnet. Default route table has no internet route.</td></tr>
</table>

<h3>Security: Security Groups vs NACLs</h3>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:15%;">Feature</th><th style="border:1px solid #555; padding:8px; text-align:left;">Security Group</th><th style="border:1px solid #555; padding:8px; text-align:left;">NACL (Network ACL)</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Level</strong></td><td style="border:1px solid #555; padding:8px;">Instance/ENI level (attached to resource)</td><td style="border:1px solid #555; padding:8px;">Subnet level (applies to all resources in subnet)</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>State</strong></td><td style="border:1px solid #555; padding:8px;">Stateful ‚Äî return traffic automatically allowed</td><td style="border:1px solid #555; padding:8px;">Stateless ‚Äî must explicitly allow return traffic (ephemeral ports)</td></tr>
</table>

<hr>

<h2>Load Balancers ‚Äî ALB vs NLB Deep Dive</h2>
<p>This is one of the most commonly tested areas. You need to know not just which to pick, but WHY and what happens under the hood.</p>

<h3>ALB vs NLB Comparison</h3>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:15%;">Feature</th><th style="border:1px solid #555; padding:8px; text-align:left;">ALB (Application Load Balancer)</th><th style="border:1px solid #555; padding:8px; text-align:left;">NLB (Network Load Balancer)</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Layer</strong></td><td style="border:1px solid #555; padding:8px;">Layer 7 (HTTP/HTTPS). Understands HTTP headers, paths, methods.</td><td style="border:1px solid #555; padding:8px;">Layer 4 (TCP/UDP/TLS). Sees only IP packets and port numbers.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Routing</strong></td><td style="border:1px solid #555; padding:8px;">Path-based (/api/* ‚Üí service A), host-based (api.x.com ‚Üí A), header/query string routing.</td><td style="border:1px solid #555; padding:8px;">Port-based only. All traffic on port 443 goes to same target group.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Latency</strong></td><td style="border:1px solid #555; padding:8px;">~1-5ms added latency (HTTP parsing, rule evaluation).</td><td style="border:1px solid #555; padding:8px;">~100-200 microseconds added. Near wire-speed.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Throughput</strong></td><td style="border:1px solid #555; padding:8px;">Scales automatically but has warm-up time. Request pre-warming for known traffic spikes.</td><td style="border:1px solid #555; padding:8px;">Millions of requests/sec. Handles sudden spikes without warm-up.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Static IP</strong></td><td style="border:1px solid #555; padding:8px;">No. Gets DNS name that resolves to changing IPs. Use Global Accelerator for stable IPs.</td><td style="border:1px solid #555; padding:8px;">Yes. One static IP per AZ. Can assign Elastic IPs.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>TLS Termination</strong></td><td style="border:1px solid #555; padding:8px;">Yes. Full HTTP/2 and gRPC support. Certificate management via ACM.</td><td style="border:1px solid #555; padding:8px;">Yes (TLS listener) but no HTTP awareness. Also supports TCP passthrough.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>WebSocket/SSE</strong></td><td style="border:1px solid #555; padding:8px;">Supports WebSocket natively. Sticky sessions available. Good for SSE.</td><td style="border:1px solid #555; padding:8px;">Supports any TCP connection. Better for long-lived connections at massive scale.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Health Checks</strong></td><td style="border:1px solid #555; padding:8px;">HTTP health checks (path, status codes, response body). Rich and flexible.</td><td style="border:1px solid #555; padding:8px;">TCP, HTTP, or HTTPS. TCP checks are faster (just connection open/close).</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Source IP</strong></td><td style="border:1px solid #555; padding:8px;">Client IP in X-Forwarded-For header. LB's own IP is what targets see.</td><td style="border:1px solid #555; padding:8px;">Preserves original client IP (for TCP). Targets see the real source IP.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Cost</strong></td><td style="border:1px solid #555; padding:8px;">~$0.0225/hour + $0.008/LCU (based on new connections, active connections, bandwidth, rules).</td><td style="border:1px solid #555; padding:8px;">~$0.0225/hour + $0.006/NLCU. Cheaper per unit at high throughput.</td></tr>
</table>

<h3>When To Use Which</h3>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:25%;">Scenario</th><th style="border:1px solid #555; padding:8px; text-align:left; width:15%;">Choice</th><th style="border:1px solid #555; padding:8px; text-align:left;">Why</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>REST/GraphQL API gateway</strong></td><td style="border:1px solid #555; padding:8px;">ALB</td><td style="border:1px solid #555; padding:8px;">Need path-based routing, HTTP headers, multiple services behind one LB.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>gRPC services</strong></td><td style="border:1px solid #555; padding:8px;">ALB</td><td style="border:1px solid #555; padding:8px;">Native HTTP/2 and gRPC support with target group routing.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Real-time event streaming (like DRIFT)</strong></td><td style="border:1px solid #555; padding:8px;">NLB</td><td style="border:1px solid #555; padding:8px;">Ultra-low latency, millions of connections, no HTTP overhead needed.</td></tr>
</table>

<hr>

<h2>Compute ‚Äî ECS vs EKS vs Lambda</h2>
<table style="width:100%; border-collapse:collapse; margin-top:10px;">
    <tr><th style="border:1px solid #555; padding:8px; text-align:left; width:12%;">Factor</th><th style="border:1px solid #555; padding:8px; text-align:left;">ECS (Fargate)</th><th style="border:1px solid #555; padding:8px; text-align:left;">ECS (EC2)</th><th style="border:1px solid #555; padding:8px; text-align:left;">EKS</th><th style="border:1px solid #555; padding:8px; text-align:left;">Lambda</th></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Ops Overhead</strong></td><td style="border:1px solid #555; padding:8px;">Minimal. No instances to manage.</td><td style="border:1px solid #555; padding:8px;">Medium. You manage the EC2 fleet + capacity.</td><td style="border:1px solid #555; padding:8px;">High. Kubernetes complexity. Control plane managed but worker nodes, networking, RBAC on you.</td><td style="border:1px solid #555; padding:8px;">None. Fully managed.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Best For</strong></td><td style="border:1px solid #555; padding:8px;">Microservices, APIs, background workers. Your default choice.</td><td style="border:1px solid #555; padding:8px;">GPU workloads, large persistent instances, cost optimization with Spot/RIs.</td><td style="border:1px solid #555; padding:8px;">Teams already invested in K8s, multi-cloud portability, complex service mesh needs.</td><td style="border:1px solid #555; padding:8px;">Event-driven, bursty, sub-1s functions, glue between services.</td></tr>
    <tr><td style="border:1px solid #555; padding:8px;"><strong>Scaling</strong></td><td style="border:1px solid #555; padding:8px;">Task-level auto-scaling. 30-60s to launch new task.</td><td style="border:1px solid #555; padding:8px;">Instance + task scaling. Slower (instance launch + task).</td><td style="border:1px solid #555; padding:8px;">HPA/VPA + Cluster Autoscaler + Karpenter. Powerful but complex.</td><td style="border:1px solid #555; padding:8px;">Instant scale to 1000s. Cold start: 100ms-1s</td></tr>
</table>
`
            },
            {
                shortcut: "J",
                title: "Job Scheduler",
                tags: ["distributed", "cron", "queuing", "reliability"],
                points: [
                    "Scheduling: Cron parser, next execution time calculation",
                    "Storage: Persistent queue (MySQL, Redis) for jobs",
                    "Execution: Worker pool with task distribution",
                    "At-least-once: Acknowledgments, retry on failure",
                    "Distributed Locks: Prevent duplicate execution",
                    "Monitoring: Job status, execution history, alerts",
                    "Examples: Celery, Sidekiq, AWS Step Functions"
                ]
            },
            {
                shortcut: "K",
                title: "Key-Value Store (Redis)",
                tags: ["caching", "data-structures", "persistence", "replication"],
                points: [
                    "In-memory: O(1) reads/writes, millions of ops/sec",
                    "Data Structures: Strings, hashes, lists, sets, sorted sets",
                    "Persistence: RDB snapshots, AOF append-only file",
                    "Replication: Master-replica for read scaling",
                    "Clustering: Hash slots, automatic sharding",
                    "Pub/Sub: Real-time messaging between services",
                    "TTL: Automatic expiration for cache entries"
                ]
            },
            {
                shortcut: "L",
                title: "LeetCode - Online Judge",
                tags: ["sandbox", "queue", "containers", "execution"],
                points: [
                    "Code Execution: Sandboxed containers (Docker/gVisor)",
                    "Security: No network, limited CPU/memory, timeout",
                    "Queue: Redis/RabbitMQ for submission processing",
                    "Workers: Auto-scaling container pool for execution",
                    "Test Cases: Stream large test cases, compare output",
                    "Languages: Pre-built images per language runtime",
                    "Plagiarism: Code similarity detection (AST comparison)"
                ]
            },
            {
                shortcut: "M",
                title: "Metrics Monitoring & Alert",
                tags: ["time-series", "alerting", "dashboards", "ingestion", "Prometheus", "Flink"],
                points: [
                    "Ingest: Agents on servers buffer & batch metrics ‚Üí Kafka ‚Üí Ingestion Consumer ‚Üí Time-Series DB",
                    "Scale: 5M metrics/sec from 500k servers, sub-second dashboard queries, alert latency <1 min",
                    "Core Entities: Metric (name), Label (key-value), Series (metric+labels over time), Alert Rule, Dashboard",
                    "Storage: Time-Series DB (InfluxDB/TimescaleDB) with time-partitioning, compression, retention tiers/rollups",
                    "Alerting: Rules in Postgres ‚Üí Evaluator polls TSDB every 1 min ‚Üí emits events ‚Üí Notification Service (dedup, grouping, silencing, escalation)",
                    "Cardinality Control: Policy store (Postgres) + Redis cardinality tracker at ingestion to cap series per metric",
                    "HA: Agents buffer locally on failure, Kafka replicates across zones, idempotent writes, alert state checkpointed"
                ],
                walkthrough: `
<h3>Phase 1: Requirements (2-3 mins)</h3>

<h4>Functional Requirements</h4>
<ul>
    <li>Services emit metrics to the platform</li>
    <li>Query/visualize metrics on dashboards</li>
    <li>Define alert rules with thresholds</li>
    <li>Receive notifications when alerts fire</li>
</ul>

<h4>Non-Functional Requirements</h4>
<ul>
    <li>Scale: 5M metrics/sec, 500k servers</li>
    <li>Dashboard queries return in seconds</li>
    <li>Alert latency < 1 minute</li>
    <li>High availability (eventual consistency ok)</li>
    <li>Handle late/out-of-order data gracefully</li>
</ul>

<h4>Core Entities</h4>
<ul>
    <li><strong>Metric:</strong> Named measurement (e.g., <code>cpu_usage</code>)</li>
    <li><strong>Label:</strong> Key-value pair for slicing (host="server-1", region="us-east")</li>
    <li><strong>Series:</strong> Unique combination of metric + labels over time ‚Äî the thing you store, query, and pay for</li>
    <li><strong>Alert Rule:</strong> Query + threshold + duration ‚Üí notification</li>
    <li><strong>Dashboard:</strong> Collection of query panels ‚Üí read amplifier</li>
</ul>

<h4>Data Flow (clarify early)</h4>
<ol>
    <li>Services generate metric data points (CPU, memory, latency, etc.) and send them to our platform</li>
    <li>Platform ingests, validates, and stores metrics as time-series data</li>
    <li>Users query stored metrics through dashboards, filtering and aggregating</li>
    <li>Alert rules periodically evaluated against stored metrics</li>
    <li>When alert condition is breached, notification sent to configured channels (Slack, etc.)</li>
</ol>

<pre>  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Services ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ  Ingest  ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ  Store   ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ  Query   ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇDashboard ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚Üì
                                                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                  ‚îÇ  Alert   ‚îÇ
                                                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚Üì
                                                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                  ‚îÇ  Notify  ‚îÇ
                                                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</pre>

<hr>

<h3>Phase 2: High-Level Design (5-6 mins)</h3>
<p><strong>Flow:</strong> Agent ‚Üí Ingest ‚Üí Kafka ‚Üí Consumers ‚Üí TSDB ‚Üí Query API ‚Üí Dashboards</p>

<h4>1. Getting Metrics from Services (Ingestion)</h4>
<p>We don't want 500k servers hitting the ingestion service directly ‚Äî we push the work to the edge.</p>
<ul>
    <li>Run a lightweight agent on each server that collects metrics locally at high frequency, buffers and batches them, then periodically flushes to the ingestion service ‚Äî this reduces 5M req/s to ~50k (100 metrics per batch)</li>
    <li>Agents also do local <strong>pre-aggregation</strong> (computing percentiles before shipping) which saves bandwidth and storage</li>
    <li>After the agent: Ingestion Service ‚Üí Kafka ‚Üí Ingestion Consumer ‚Üí Storage</li>
    <li>Kafka helps buffer potential spikes and decouples ingestion from storage writes. If consumers fall behind, data sits safely in Kafka rather than getting dropped</li>
</ul>

<pre>Servers (500k, with Agents) ‚Üí Ingestion Service ‚Üí Kafka ‚Üí Ingestion Consumer ‚Üí Time-Series DB</pre>

<h4>2. Querying & Dashboards</h4>
<ul>
    <li>Relational DB won't work at this scale ‚Äî need a <strong>time-series database</strong> (InfluxDB, TimescaleDB, VictoriaMetrics) because the workload is append-only, time-partitioned, and compresses well columnar</li>
    <li><strong>Dashboards are bursty, incident-driven reads:</strong> filter + group-by + aggregate over time ranges, often needing sub-second responses</li>
    <li><strong>Store metrics in a time-series optimized backend</strong> with time-based partitioning, compression, and a label‚Üíseries index</li>
    <li><strong>Use retention tiers/rollups</strong> so long-range queries are cheap while recent data stays high-res</li>
    <li>Front it with a <strong>query layer</strong> that adds caching + query limits (timeouts, max series, concurrency)</li>
    <li><strong>Isolate/prioritize alerting reads</strong> so dashboard spikes don't break paging</li>
</ul>

<pre>                                                          ‚îå ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îÄ ‚îê
                                                            Multi-resolution
  Servers                                                   ‚îÇ    rollups      ‚îÇ
  ‚îå‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê                                                   ‚îî ‚îÄ ‚îÄ ‚îÄ ‚îÄ‚î¨‚îÄ ‚îÄ ‚îÄ ‚îÄ‚îò
  ‚îÇ  ‚îÇ  ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  ‚îÇ  ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ Ingestion ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ Kafka ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ Ingestion ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ  Time-Series   ‚îÇ
  ‚îî‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò    ‚îÇ  Service  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ Consumer  ‚îÇ    ‚îÇ      DB        ‚îÇ
 (with       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  Agents)                                                      query ‚Üì
                                                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                           ‚îÇ  Query Service ‚îÇ
                                                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                   ‚Üì
                                                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                           ‚îÇ   Dashboard    ‚îÇ
                                                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</pre>

<h4>3. Alerting & Notification ‚Äî Two Separate Systems</h4>
<p><strong>Key design decision:</strong> Evaluation and notification are deliberately separate systems. Different scaling/reliability profiles.</p>

<p><strong>i. Alert Evaluation:</strong></p>
<ol>
    <li>Alert rules stored in <strong>Postgres</strong> (simple CRUD, low volume)</li>
    <li>Alert evaluator <strong>polls, not streams</strong> ‚Äî runs every 1 minute, grabs all rules from Postgres, queries TSDB to evaluate each one</li>
    <li><strong>Why polling?</strong> Simple, battle-tested ‚Äî this is how Prometheus Alertmanager works</li>
    <li>When a rule breaches ‚Üí <strong>emit an event.</strong> Don't call Slack/PagerDuty directly</li>
</ol>

<p><strong>ii. Notification Service:</strong></p>
<ol>
    <li><strong>Deduplication:</strong> Track alert state (firing/resolved), only notify on state transitions. No repeat pages for the same ongoing incident</li>
    <li><strong>Grouping:</strong> Collect alerts in a short window, group by label, send one notification per group</li>
    <li><strong>Silencing:</strong> Mute during maintenance windows</li>
    <li><strong>Escalation:</strong> Re-notify through a different channel if nobody acknowledges within configured time</li>
</ol>

<pre>  Servers
  ‚îå‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  ‚îÇ  ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ Ingestion ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ Kafka ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ Ingestion ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ  Time-Series   ‚îÇ
  ‚îî‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îò    ‚îÇ  Service  ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ Consumer  ‚îÇ    ‚îÇ      DB        ‚îÇ
 (with       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  Agents)                                                      query ‚Üì
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ Alerts   ‚îÇ ‚Üê‚îÄ‚îÄ‚îÇ  Alert    ‚îÇ ‚Üê‚îÄ‚îÄ query ‚îÄ‚îÄ ‚îÇ  Query Service ‚îÇ
                 ‚îÇ   DB     ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ  Service  ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚Üì
                                  alert fires              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                       ‚Üì                   ‚îÇ   Dashboard    ‚îÇ
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ  Noti    ‚îÇ ‚Üê‚îÄ‚îÄ‚îÇ   Noti    ‚îÇ
                 ‚îÇ   DB     ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ  Service  ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò
                                    ‚Üì  ‚Üì  ‚Üì
                                Slack SMS Pager</pre>

<hr>

<h3>Phase 3: Deep Dives (6-8 mins)</h3>

<h4>Deep Dive A: Real-Time Alerts (sub-minute)</h4>
<p><strong>If polling every 1 minute isn't fast enough:</strong></p>
<ul>
    <li>Bring in <strong>Flink</strong> to stream directly from Kafka</li>
    <li>Alert rules compile into Flink operators that continuously evaluate conditions</li>
    <li>Threshold violation ‚Üí Flink emits alert event immediately</li>
    <li><strong>Trade-off:</strong> Flink adds operational complexity ‚Äî rules must be translated into streaming operators</li>
</ul>

<h4>Deep Dive B: High Availability</h4>
<p><strong>Approach:</strong> Keep the design simple, but make every step resumable</p>
<p><strong>Ingestion path:</strong></p>
<ol>
    <li>Agents buffer locally and retry if network or ingestion layer is down</li>
    <li>Kafka is replicated across zones so a broker loss doesn't drop data</li>
    <li>Writes are idempotent so retries don't create duplicate points</li>
</ol>
<p><strong>Alerting + notification path:</strong></p>
<ol>
    <li>Alert evaluation state is checkpointed so a processor crash can resume</li>
    <li>Alert events are written to Kafka before any external notifications</li>
    <li>Alert Service retries delivery and can fail over to a secondary channel</li>
</ol>

<h4>Deep Dive C: Cardinality Explosion</h4>
<p><strong>The problem:</strong> Each unique combo of metric + labels = a new series. A single metric like <code>http_requests</code> with 5 label dimensions across realistic infrastructure can produce 50M series. Hurts both writes (index/memory overhead per series) and reads (aggregating across millions of series is slow).</p>
<p><strong>The solution ‚Äî enforce at ingest with two components:</strong></p>
<ol>
    <li><strong>Policy store (Postgres):</strong> Defines per-metric rules ‚Äî which labels are allowed, max series count, per-label value limits. E.g., <code>http_requests</code> capped at 500k series</li>
    <li><strong>Cardinality tracker (Redis):</strong> Tracks unique series per metric in real-time using a set. New label combo comes in ‚Üí check if it exists ‚Üí if new, check against cap ‚Üí reject if over limit</li>
</ol>
<p>Both sit in the ingestion service between validation and Kafka publish ‚Äî so bad data never enters the pipeline.</p>
`
            },
            {
                shortcut: "N",
                title: "Noisy Neighbor ‚Äî CARL Format",
                tags: ["incident", "DRIFT", "throttling", "cellular", "resilience", "quotas"],
                points: [
                    "Context: DRIFT platform P2 ‚Äî single producer deployed code creating connection per request, went 50‚Üí5000, saturated broker pools",
                    "Action: Identified root cause via per-client metrics, throttled offending producer, owned stakeholder comms",
                    "Systemic: Automated quotas, cellular isolation (blast radius), leading indicator alerting (< 60s detection)",
                    "Result: 45 min stabilization, zero platform incidents since, two issues auto-throttled before impact",
                    "Learning: Multi-tenant platforms need infra-layer governance; quotas aren't punitive ‚Äî they're protective"
                ],
                walkthrough: `
<h3>Noisy Neighbor ‚Äî CARL Format</h3>

<h4>Context</h4>
<ul>
    <li>DRIFT: shared event platform, 300+ producers, 2B events daily</li>
    <li>P2 incident during end-of-day processing</li>
    <li>Platform rejecting connections across all tenants</li>
    <li>Root cause: single producer deployed code creating connection per request</li>
    <li>Went from 50 to 5,000 connections in minutes</li>
    <li>Saturated broker connection pools ‚Äî everyone suffered</li>
</ul>

<h4>Action</h4>
<ul>
    <li><strong>Immediate:</strong> My SRE team identified root cause via per-client metrics. I made the call to throttle the offending producer ‚Äî controversial but necessary. Owned stakeholder communication.</li>
    <li><strong>Systemic ‚Äî three requirements I set:</strong>
        <ul>
            <li>Automated quotas with self-service upgrades: Standard tier default, Resilience Scorecard for increases (Deployed)</li>
            <li>Cellular isolation: Blast radius contained to single cell (Deployed)</li>
            <li>Leading indicator alerting: Catch anomalies before saturation (Deployed)</li>
        </ul>
    </li>
</ul>

<h4>Result</h4>
<ul>
    <li>45 minutes to stabilization</li>
    <li>Brief degradation, no data loss</li>
    <li>Zero platform-level incidents since</li>
    <li>Two issues automatically throttled before impact</li>
    <li>Detection within 60 seconds</li>
</ul>

<h4>Learning</h4>
<ul>
    <li>Multi-tenant platforms require governance at the infrastructure layer</li>
    <li>Can't rely on producers to behave ‚Äî platform must protect itself</li>
    <li>Moved from monolithic to cellular architecture</li>
    <li>Quotas aren't punitive ‚Äî they're protective</li>
</ul>
`
            },
            {
                shortcut: "O",
                title: "Object Storage (S3)",
                tags: ["blob", "durability", "scalability", "metadata"],
                points: [
                    "Architecture: Flat namespace, bucket + key addressing",
                    "Durability: 11 9s via replication across AZs",
                    "Consistency: Strong read-after-write consistency",
                    "Versioning: Keep multiple versions of objects",
                    "Lifecycle: Transition to cheaper storage classes",
                    "Access: Pre-signed URLs, bucket policies, ACLs",
                    "Metadata: Custom headers, tagging for organization"
                ]
            },
            {
                shortcut: "P",
                title: "Payment System (Stripe)",
                tags: ["transactions", "idempotency", "PCI", "ledger"],
                points: [
                    "Idempotency: Idempotency keys prevent duplicate charges",
                    "Ledger: Double-entry bookkeeping, immutable records",
                    "PCI Compliance: Tokenization, never store raw card data",
                    "Retry: Exponential backoff for failed charges",
                    "Webhooks: Async notifications for payment events",
                    "Fraud Detection: ML models, velocity checks",
                    "Multi-currency: Real-time exchange rates, settlement"
                ]
            },
            {
                shortcut: "Q",
                title: "Queue System (SQS)",
                tags: ["async", "decoupling", "dead-letter", "visibility"],
                points: [
                    "Decoupling: Producer and consumer independent scaling",
                    "Visibility Timeout: Message hidden while processing",
                    "Dead Letter Queue: Failed messages after N retries",
                    "FIFO: Exactly-once, ordered processing (with dedup)",
                    "Long Polling: Reduce empty responses, lower cost",
                    "Batch: Send/receive up to 10 messages at once",
                    "Scaling: Auto-scale consumers based on queue depth"
                ]
            },
            {
                shortcut: "R",
                title: "Rate Limiter",
                tags: ["throttling", "algorithms", "distributed", "API", "Redis", "token-bucket"],
                points: [
                    "Scope: Server-side rate limiting for social media API ‚Äî limiting individual HTTP requests, not business operations",
                    "Identify clients by User ID, IP address, or API key; limit via configurable rules (e.g., 100 req/min/user)",
                    "Algorithm: Token Bucket via Redis preferred ‚Äî handles bursty traffic with global state across gateways",
                    "Atomicity: Lua script in Redis wraps read-calculate-update to prevent race conditions (double-spend)",
                    "Scale: 1M RPS via Redis sharding + consistent hashing (10-20 shards), <10ms latency overhead",
                    "HA: Master-replica replication with automatic failover; fail-closed preferred for high-traffic platforms",
                    "Reject with HTTP 429 + headers (X-RateLimit-Limit, Remaining, Reset)"
                ],
                walkthrough: `
<h3>Phase 1: Requirements (2-3 mins)</h3>

<h4>Functional Requirements</h4>
<p>Rate limiter for a social media platform's API:</p>
<ul>
    <li>Limiting individual HTTP requests (posting tweets, fetching timelines, uploading photos)</li>
    <li>No high-level business operations ‚Äî focus is on server-side traffic control</li>
</ul>

<p><strong>Core Requirements:</strong></p>
<ol>
    <li>Identify clients by <strong>User ID, IP address, or API key</strong> to apply appropriate limits</li>
    <li>Limit HTTP requests based on <strong>configurable rules</strong> (e.g., 100 API requests per minute per user)</li>
    <li>When limits exceeded, reject with <strong>HTTP 429</strong> and helpful headers (rate limit remaining, reset time)</li>
</ol>

<h4>Non-Functional Requirements</h4>
<p><em>Best to ask interviewer about scale expectations.</em></p>
<ul>
    <li>Minimal latency overhead (<strong>&lt;10ms</strong> per request check)</li>
    <li>Highly available ‚Äî eventual consistency ok (slight delays in limit enforcement across nodes acceptable)</li>
    <li><strong>1M requests/second</strong> across 100M DAU</li>
</ul>

<h4>Core Entities</h4>
<ul>
    <li><strong>Rules:</strong> Rate limiting policies ‚Äî requests per time window, which clients it applies to, what endpoint it covers</li>
    <li><strong>Clients:</strong> Entities being rate limited ‚Äî users (user ID), IP addresses, API keys, or combination</li>
    <li><strong>Requests:</strong> Incoming API requests evaluated against rules ‚Äî carries client identity, endpoint, timestamp</li>
</ul>

<h4>System Interface</h4>
<p>A rate limiter is an infrastructure component that other services call to check if a request should be allowed:</p>
<pre>isRequestAllowed(clientId, ruleId) ‚Üí { passes: boolean, remaining: number, resetTime: timestamp }</pre>
<p>Response includes headers: <code>X-RateLimit-Remaining</code> and <code>X-RateLimit-Reset</code></p>

<hr>

<h3>Phase 2: High-Level Design (5-6 mins)</h3>

<h4>1. Where does the rate limiter live?</h4>
<p>At the <strong>API Gateway</strong> ‚Äî determines what info we have access to and how it integrates. The rate limiter only has access to info in the HTTP request: URL/path, headers (Authorization, User-Agent, X-API-Key), query params, and client IP.</p>

<p><strong>Client identification options:</strong> User ID, IP address, or API key</p>

<h4>2. Rate Limiting Algorithms</h4>
<p>Four main algorithms used in production:</p>
<ul>
    <li><strong>Fixed Window:</strong> Split time into fixed windows, keep per-user counter per window. Reject if exceeded until next window</li>
    <li><strong>Sliding Window:</strong> Maintain list of request timestamps per user. Purge timestamps older than window (e.g., 60s) and count remaining</li>
    <li><strong>Sliding Window Counter:</strong> Hybrid approach combining fixed and sliding</li>
    <li><strong>Token Bucket:</strong> Each client has a bucket of tokens (burst capacity). Tokens refill at steady rate. Each request spends 1 token. No tokens ‚Üí reject</li>
</ul>

<h4>3. Preferred: Token Bucket via Redis</h4>
<p><strong>Why:</strong> Handles bursty traffic while maintaining global state across multiple servers.</p>

<p><strong>Core logic:</strong> Instead of each API gateway keeping a local count, they all reference a centralized Redis store. If user requests split across gateways, total usage is still tracked against a single "global bucket."</p>

<p><strong>Step-by-step flow:</strong></p>
<ol>
    <li><strong>Trigger:</strong> Request arrives at gateway with specific User ID</li>
    <li><strong>State Retrieval:</strong> Gateway fetches from Redis: current token count + timestamp of last refill</li>
    <li><strong>Calculation:</strong> Calculate new tokens earned based on elapsed time (e.g., 1 token/sec). Add to existing count, capped at max bucket capacity</li>
    <li><strong>Decision:</strong>
        <ul>
            <li>tokens ‚â• 1 ‚Üí allowed, remove one token, save new count/timestamp to Redis</li>
            <li>tokens &lt; 1 ‚Üí rejected (rate limited)</li>
        </ul>
    </li>
</ol>

<p><strong>Key Technical Hurdle ‚Äî Atomicity:</strong></p>
<ul>
    <li><strong>Problem:</strong> Standard "read-then-write" creates a race condition ‚Äî two simultaneous requests see the same token and "double-spend" it</li>
    <li><strong>Solution:</strong> Wrap the entire read-calculate-update into a <strong>Lua script</strong></li>
    <li><strong>Why it works:</strong> Redis executes Lua scripts atomically. No other command can run mid-script, effectively locking the bucket for that microsecond ‚Äî 100% race-condition free</li>
</ul>

<h4>4. Rejection Handling (HTTP 429)</h4>
<ul>
    <li>Failed requests get bounced immediately ‚Äî no queue, no retry. Send 429 with helpful headers</li>
    <li>Headers: <code>X-RateLimit-Limit</code>, <code>X-RateLimit-Remaining</code>, <code>X-RateLimit-Reset</code></li>
</ul>

<pre>                           Client
                             ‚Üì
                       API Gateway
                    (Rate Limiter here)
                          ‚Üì    ‚Üì
                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                   ‚îÇ Rule ‚îÇ    ‚îÇ Token Bucket  ‚îÇ
                   ‚îÇ Store‚îÇ    ‚îÇ   (Redis)     ‚îÇ
                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚Üì
                          Allowed? ‚îÄ‚îÄ‚Üí Backend Services
                          Denied?  ‚îÄ‚îÄ‚Üí HTTP 429 + Headers</pre>

<hr>

<h3>Phase 3: Deep Dives (6-8 mins)</h3>

<h4>Deep Dive A: Scaling to 1M RPS</h4>
<p><strong>Bottleneck:</strong> Single Redis instance handles 100-200k RPS. We need 1M RPS ‚Äî one instance won't work.</p>
<p><strong>Solution ‚Äî Sharding & Consistent Hashing:</strong></p>
<ul>
    <li>Partition rate limiting data across multiple Redis shards</li>
    <li><strong>Partition key:</strong> Hash the client's unique identifier (User ID, IP, API key) to determine which shard stores their bucket</li>
    <li>Consistent hashing ensures all requests for a specific client always hit the same shard</li>
    <li><strong>Linear scalability:</strong> Add more shards ‚Üí capacity increases linearly. 10-20 shards sufficient for 1M RPS</li>
</ul>

<h4>Deep Dive B: High Availability & Fault Tolerance</h4>
<p><strong>Failure mode ‚Äî Fail-open vs Fail-closed:</strong></p>
<ul>
    <li><strong>Fail-closed preferred</strong> for high-traffic platforms ‚Äî brief rejected requests are safer than total system collapse during viral events</li>
</ul>
<p><strong>Proactive prevention ‚Äî HA design:</strong></p>
<ul>
    <li><strong>Master-Replica replication:</strong> Data replicated to standby nodes</li>
    <li><strong>Automatic failover:</strong> Redis Cluster detects master failure and promotes a replica to new master without manual intervention</li>
</ul>

<h4>Deep Dive C: Hot Keys</h4>
<p><strong>Problem:</strong> User keeps hammering after exhausting their token bucket allocation.</p>
<ul>
    <li><strong>Automatic blocking:</strong> Stop checking Redis after N consecutive rejections ‚Äî short-circuit locally</li>
    <li><strong>Edge-layer DDoS protection:</strong> Prevent abusive traffic from consuming infrastructure resources, ensuring legitimate users get fast rate limit checks</li>
</ul>

<h4>Deep Dive D: Minimizing Latency Overhead</h4>
<p>Every check requires a network round trip to Redis. At 1M RPS, latency adds up.</p>
<ul>
    <li><strong>Connection Pooling (quickest win):</strong> Maintain a warm pool of persistent TCP connections instead of opening/closing per check ‚Äî skip the costly handshake</li>
    <li><strong>Geographic Distribution (biggest win):</strong> Distance = latency. If user is in Tokyo and Redis is in Virginia, physics slows you down. Deploy API gateways and Redis clusters in multiple regions close to end users</li>
</ul>

<h4>Deep Dive E: Dynamic Rule Configuration</h4>
<p>Rules are dynamic ‚Äî temporarily increase limits during product launches, premium users get higher limits.</p>
<ul>
    <li><strong>Push-based configuration:</strong> Zookeeper maintains config data and notifies all connected API Gateways immediately when any configuration changes</li>
</ul>

<h4>Deep Dive F: Observability & Alerting</h4>
<p><strong>Metrics:</strong> p99 rate limit check latency, rejection rate per endpoint, Redis CPU/memory</p>
<p><strong>Dashboard:</strong> Top rejected IPs/users (abuse patterns), rate limit hit distribution, Redis cluster health</p>
<p><strong>Alerts:</strong></p>
<ul>
    <li>Rate limit check latency &gt; 10ms for 5 min</li>
    <li>Single IP/user &gt; 10k requests/min</li>
    <li>Redis failover events</li>
</ul>
<p><strong>Additional considerations:</strong> Graceful degradation, gradual rollout strategy, multi-region consistency (primary for writes, async replication to read replicas, accept eventual consistency)</p>
`
            },
            {
                shortcut: "S",
                title: "Scout Engine ‚Äî Operational Intelligence from Noise",
                tags: ["ML", "incidents", "classification", "AIOps", "ITSM", "platform"],
                points: [
                    "Context: Millions of ITSM events, data fragmented, critical signals buried in unstructured free text",
                    "Problem: Same failures described inconsistently, no reliable way to correlate changes ‚Üî incidents, reactive firefighting",
                    "Action: Time-boxed experiment ‚Äî 1 engineer, 1 month, ‚Ç¨0 budget. Built lightweight classifier to group similar tickets",
                    "Scaling: Secured funding ‚Üí Data Science (model training), Product (insights), Me (platform & operations)",
                    "Platform: Three-layer architecture ‚Äî ingestion, classification, visibility",
                    "Results: 5 services caused ~40% of incidents, 30% reduction in recurring incidents, 55% faster triage",
                    "Key Learning: Leverage wasn't ML ‚Äî it was shared intelligence layer aligning ops, product, and leadership"
                ],
                walkthrough: `
<h3>Scout (Operational Intelligence from Noise)</h3>

<h4>Context</h4>
<ul>
    <li>Millions of ITSM events (incidents, changes, problems)</li>
    <li>Data fragmented; critical signals buried in unstructured free text</li>
    <li>Same failure described inconsistently; manual classification didn't scale
        <ul><li>e.g., DB Timeout, ORA-12170</li></ul>
    </li>
    <li>No reliable way to correlate <strong>changes ‚Üî incidents</strong> or spot emerging patterns</li>
    <li>Result: reactive firefighting, risk escalation, wasted engineering effort
        <ul><li>I identified a structural gap no one owned yet</li></ul>
    </li>
</ul>

<h4>Action</h4>
<ul>
    <li>Ran a time-boxed experiment: <strong>1 engineer, 1 month, ‚Ç¨0 budget</strong></li>
    <li>Goal wasn't production ML ‚Äî just to validate whether signal existed</li>
    <li>Built lightweight classifier to group similar tickets</li>
    <li>Presented findings to my MD which coincided (during an AIOps vendor pitch)</li>
    <li>Demonstrated that our highest-value insights came from internal context</li>
    <li>Secured funding and formalized ownership:
        <ul>
            <li>Data Science ‚Üí model training</li>
            <li>Product ‚Üí Insights</li>
            <li>Me ‚Üí platform and operations</li>
        </ul>
    </li>
    <li>Scaled into a three-layer platform: ingestion, classification, visibility</li>
</ul>

<h4>Results</h4>
<ul>
    <li>Identified recurring failure themes and <strong>incident hotspots</strong></li>
    <li><strong>5 services caused ~40% of incidents</strong> (clear risk concentration)</li>
    <li><strong>30% reduction</strong> in recurring incidents via targeted fixes</li>
    <li><strong>55% faster triage</strong> using similar-incident detection and resolution history</li>
    <li>Shifted from reactive response to <strong>proactive intervention</strong></li>
</ul>

<h4>Key Learnings</h4>
<ul>
    <li>High-impact innovation doesn't require big teams or budgets
        <ul><li>Prove value fast, then invest</li></ul>
    </li>
    <li>The leverage wasn't ML; it was shared intelligence layer aligning:
        <ul>
            <li>ops</li>
            <li>product</li>
            <li>leadership</li>
        </ul>
    </li>
</ul>

<hr>

<h4>Architecture Flow</h4>
<pre>[Data Sources]
        ‚Üì
[DRIFT ‚Äî Event Streaming]
        ‚Üì
[SCOUT ‚Äî ML Processing]
        ‚Üì
[Outputs: Alerts, Dashboards, Correlations]</pre>

<hr>

<h4>Walk Through Each Layer</h4>

<h4>1. Data Ingestion</h4>
<blockquote>
"SCOUT sits on top of DRIFT ‚Äî our event streaming platform. We ingest:
<ul>
    <li>Incident tickets from ServiceNow</li>
    <li>Change records</li>
    <li>Container and VM telemetry</li>
    <li>Application metrics</li>
    <li>Deployment events</li>
</ul>
All normalized into a common schema, flowing through Kafka."
</blockquote>

<h4>2. Correlation Engine</h4>
<blockquote>
"First layer is correlation ‚Äî linking related events. When an incident fires, we automatically ask: what changed in the last hour? What services are affected? What's the dependency chain?<br>
This used to take 30 minutes of human investigation. Now it's seconds."
</blockquote>

<h4>3. ML Models</h4>
<blockquote>
"On top of correlation, we run ML models:
<ul>
    <li><strong>Anomaly detection:</strong> Baseline normal behavior, flag deviations before they become incidents</li>
    <li><strong>Pattern recognition:</strong> Have we seen this failure signature before? What fixed it last time?</li>
    <li><strong>Risk scoring:</strong> This change touches a high-risk service with no DR plan ‚Äî flag it"</li>
</ul>
</blockquote>

<h4>4. Outputs</h4>
<blockquote>
"Three main outputs:
<ul>
    <li><strong>Proactive alerts:</strong> Anomaly detected before users notice</li>
    <li><strong>Enriched incidents:</strong> Incident ticket auto-populated with correlated changes, affected services, similar past incidents</li>
    <li><strong>Dashboards:</strong> Operational health view ‚Äî what's at risk, what's degraded, what changed"</li>
</ul>
</blockquote>

<hr>

<h4>Sketch If Asked</h4>
<pre>[ServiceNow]  ‚îÄ‚îÄ‚îÄ‚îê
[Change Mgmt] ‚îÄ‚îÄ‚îÄ‚î§‚îÄ‚îÄ‚Üí [DRIFT/Kafka] ‚Üí [Correlation Engine] ‚Üí [ML Models] ‚Üí [Alerts]
[Telemetry]   ‚îÄ‚îÄ‚îÄ‚î§                            ‚Üì                     ‚Üì
[Metrics]     ‚îÄ‚îÄ‚îÄ‚îò                       [Graph DB]        [Enriched Incidents]
                                       (Dependencies)              ‚Üì
                                                             [Dashboards]</pre>

<hr>

<h4>Results (Land the Impact)</h4>
<blockquote>
"Outcomes:
<ul>
    <li>MTTR reduced ‚Äî investigations that took hours now take minutes</li>
    <li>Proactive detection ‚Äî we catch anomalies before P1s</li>
    <li>Change correlation ‚Äî 80% of incidents now auto-linked to a causal change</li>
    <li>Reduced toil ‚Äî SRE team shifted from manual correlation to reviewing recommendations"</li>
</ul>
</blockquote>

<hr>

<h4>Director-Level Ownership Framing</h4>
<blockquote>
"My team owns the platform ‚Äî data ingestion, ML infrastructure, model serving. Data science partners own model development. Service teams consume the outputs. I own the outcomes."
</blockquote>

<hr>

<h4>If They Probe Deeper</h4>
<table>
    <tr><th>Probe</th><th>Your Answer</th></tr>
    <tr><td>"What ML models?"</td><td>"Anomaly detection using statistical baselines initially, exploring deep learning for pattern matching. My data science partners own model selection ‚Äî I own the infrastructure to serve them."</td></tr>
    <tr><td>"What's the latency?"</td><td>"Near real-time for correlation ‚Äî seconds. ML inference is sub-second. Alert firing within a minute of anomaly."</td></tr>
    <tr><td>"How do you handle false positives?"</td><td>"Feedback loop. Teams can dismiss alerts, that feeds back into model training. We tuned aggressively early ‚Äî precision over recall."</td></tr>
    <tr><td>"What's the hardest part?"</td><td>"Data quality. Garbage in, garbage out. We spent significant effort on schema normalization and data validation before ML added any value."</td></tr>
</table>
`
            },
            {
                shortcut: "T",
                title: "Ticketmaster - Booking System",
                tags: ["concurrency", "booking", "search", "caching", "Redis", "Elasticsearch"],
                points: [
                    "Functional: View events, search events, book tickets to events",
                    "NFR: Availability for search/view, consistency for booking, 10M users per popular event, <500ms search, read-heavy 100:1",
                    "Entities: Events, Users, Venues, Performers, Tickets (status: available/booked), Bookings",
                    "Ticket Reservation: Distributed lock in Redis with TTL ‚Äî hold ticket 10 min, auto-release if unpurchased",
                    "Scaling Reads: Cache event details (high read, low update) with DB triggers for invalidation + TTL policy",
                    "Search: Elasticsearch synced from Postgres via CDC for low-latency fuzzy search",
                    "Architecture: API Gateway ‚Üí Event Service / Search Service / Booking Service ‚Üí PostgreSQL + Redis + Stripe"
                ],
                walkthrough: `
<h3>Phase 1: Requirements (2-3 mins)</h3>

<h4>Functional Requirements</h4>
<ol>
    <li>Users should be able to view events</li>
    <li>Users should be able to search for events</li>
    <li>Users should be able to book tickets to events</li>
</ol>

<h4>Non-Functional Requirements</h4>
<ul>
    <li>Prioritize <strong>availability</strong> for searching & viewing events, but <strong>consistency</strong> for booking events</li>
    <li>Scalable for high throughput ‚Äî popular events (10 million users, one event)</li>
    <li>Low-latency search (<500ms)</li>
    <li>Read-heavy ‚Äî needs to support high read throughput (100:1)</li>
</ul>

<h4>Core Entities</h4>
<ul>
    <li>Events, Users, Venues, Performers, Tickets, Bookings</li>
</ul>

<h4>API / System Interface</h4>
<pre>GET  /events/:eventId         ‚Üí Event & Venue
GET  /events/search?keyword=  ‚Üí Partial&lt;Event&gt;[]
POST /bookings/:eventId       ‚Üí Booking</pre>

<hr>

<h3>Phase 2: High-Level Design (3-4 mins)</h3>
<p><strong>Three core user flows:</strong></p>
<ul>
    <li>Users should be able to <strong>view</strong> events</li>
    <li>Users should be able to <strong>search</strong> for events</li>
    <li>Users should be able to <strong>book</strong> events</li>
</ul>

<pre>                         Client
                           ‚Üì
                    API Gateway
              (auth, rate limiting, routing)
                     ‚Üì         ‚Üì         ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Event   ‚îÇ ‚îÇ  Search  ‚îÇ ‚îÇ   Booking    ‚îÇ
              ‚îÇ Service  ‚îÇ ‚îÇ Service  ‚îÇ ‚îÇ   Service    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì            ‚Üì            ‚Üì     ‚Üì
              PostgreSQL   PostgreSQL   PostgreSQL  Stripe

  GET /events/:eventId     GET /events/search?keyword=   POST /bookings/:eventId</pre>

<p><strong>Database Schema (PostgreSQL):</strong></p>
<ul>
    <li><strong>Event:</strong> id, venueId, performerId, tickets[], name, description</li>
    <li><strong>Venue:</strong> id, location, seatMap</li>
    <li><strong>Performer:</strong> id, ...</li>
    <li><strong>Ticket:</strong> id, eventId, seat, price, status (available/booked), userId</li>
    <li><strong>Booking:</strong> id, userId, tickets</li>
</ul>

<hr>

<h3>Phase 3: Deep Dives (6-8 mins)</h3>

<h4>Deep Dive A: Ticket Reservation (Distributed Lock)</h4>
<p><strong>How do we improve the booking experience by reserving tickets?</strong></p>
<ul>
    <li>Use a <strong>distributed lock with Redis + TTL</strong></li>
    <li>When a user selects a ticket, acquire a lock in Redis using a unique identifier (ticket ID) with a predefined TTL (e.g., 10 min). This TTL acts as an automatic expiration time for the lock</li>
    <li>User completes purchase ‚Üí ticket status updated to "Booked" in the database, lock in Redis manually released before TTL expires</li>
    <li>If TTL expires ‚Üí Redis automatically releases the lock. Ticket becomes available for booking by other users</li>
</ul>

<h4>Deep Dive B: Scaling the View API (10s of millions of concurrent requests)</h4>

<p><strong>Caching:</strong></p>
<ul>
    <li>Prioritize caching for data with high read rates and low update frequency ‚Äî event details, performer bios, etc.</li>
    <li><strong>Cache invalidation and consistency:</strong>
        <ul>
            <li>Set up database triggers to notify the caching system of data changes (e.g., updates in event dates or performer lineups) to invalidate relevant cache entries</li>
            <li>Implement a TTL policy for cache entries, ensuring periodic refreshes</li>
        </ul>
    </li>
</ul>

<p><strong>Load Balancing:</strong></p>
<ul>
    <li>Round Robin or Least Connections for even traffic distribution across server instances</li>
</ul>

<p><strong>Horizontal Scaling:</strong></p>
<ul>
    <li>Event service is stateless ‚Äî horizontally scale by adding more instances and load balancing between them</li>
</ul>

<h4>Deep Dive C: Low-Latency Search</h4>
<p><strong>How can we improve search to meet our low-latency requirements?</strong></p>
<ul>
    <li>Use a full-text search engine like <strong>Elasticsearch</strong></li>
    <li>Sync data from Postgres to Elasticsearch using <strong>CDC (Change Data Capture)</strong></li>
    <li>Index on name, description, venue, performer, date, etc.</li>
    <li>Enable fuzzy search functionality with Elasticsearch</li>
</ul>

<pre>                              Client
                                ‚Üì
                 Virtual Waiting Queue (peak traffic)
                                ‚Üì
                          API Gateway
                    ‚Üì           ‚Üì           ‚Üì
             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
             ‚îÇ  Search   ‚îÇ ‚îÇ   Event   ‚îÇ ‚îÇ   Booking   ‚îÇ
             ‚îÇ  Service  ‚îÇ ‚îÇ  Service  ‚îÇ ‚îÇ   Service   ‚îÇ
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
                   ‚Üì          ‚Üì    ‚Üì        ‚Üì      ‚Üì
            Elasticsearch   Redis  DB   Ticket    Stripe
            (fuzzy search) (cache)  ‚Üë   Lock
                   ‚Üë               ‚îÇ   (Redis,
               CDC sync ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    TTL 10m)
                                        ‚Üì
                                   PostgreSQL

  GET /search?term=    GET /events/:id    POST /bookings/:eventId</pre>
`
            },
            {
                shortcut: "U",
                title: "Uber - Ride Sharing",
                tags: ["geolocation", "matching", "ETA", "pricing"],
                points: [
                    "Location: Geohash or QuadTree for spatial indexing",
                    "Matching: Score drivers by distance, rating, ETA",
                    "Driver Tracking: Update location every 4 sec via WebSocket",
                    "ETA: Graph algorithms (Dijkstra) + ML for traffic",
                    "Surge Pricing: Supply/demand ratio per geohash cell",
                    "Dispatch: Push notification to nearest available drivers",
                    "Trip Storage: Append-only log, batch to data warehouse"
                ]
            },
            {
                shortcut: "V",
                title: "Video Upload (YouTube)",
                tags: ["encoding", "chunks", "CDN", "processing"],
                points: [
                    "Chunked Upload: Resumable uploads for large files",
                    "Processing Pipeline: Transcode to multiple formats/resolutions",
                    "Storage: Raw in S3, processed to CDN origins",
                    "Thumbnail: Auto-generate or custom upload",
                    "Metadata: Title, description, tags in relational DB",
                    "Content Moderation: ML for policy violations",
                    "Monetization: Ad insertion, revenue tracking"
                ]
            },
            {
                shortcut: "W",
                title: "Web Crawler",
                tags: ["distributed", "politeness", "deduplication", "frontier"],
                points: [
                    "Frontier: Priority queue of URLs to crawl (BFS/PageRank)",
                    "Politeness: Respect robots.txt, rate limit per domain",
                    "Deduplication: Bloom filter or MinHash for seen URLs",
                    "Content Hashing: Detect duplicate pages (SimHash)",
                    "DNS Cache: Local DNS resolver to reduce lookups",
                    "Storage: Distributed file system for raw HTML",
                    "Scale: 1B pages = partition by domain, horizontal scaling"
                ]
            },
            {
                shortcut: "X",
                title: "X (Twitter) Timeline",
                tags: ["fanout", "trends", "real-time", "caching"],
                points: [
                    "Fanout: Hybrid model like Facebook Feed",
                    "Tweets: Append-only, sharded by user_id",
                    "Trends: Streaming aggregation of hashtags (Flink/Storm)",
                    "Search: Real-time indexing with Elasticsearch",
                    "Caching: Home timeline cached, invalidate on new tweet",
                    "Notifications: Real-time via WebSocket",
                    "Rate Limiting: Per-user, per-app quotas"
                ]
            },
            {
                shortcut: "Y",
                title: "Dropbox ‚Äî File Storage",
                tags: ["file-storage", "sync", "chunking", "S3", "sharing", "upload"],
                points: [
                    "Upload/Download: Files from any device, up to 50GB",
                    "Sharing: Share files with other users, view shared files",
                    "Sync: Auto-sync files across devices",
                    "Chunking: Split large files into chunks for parallel upload",
                    "Storage: S3 for file data, DB for metadata",
                    "Availability over consistency (CAP)",
                    "Security: Encryption at rest + in transit, file recovery"
                ],
                walkthrough: `
<h3>Dropbox ‚Äî System Design Walkthrough</h3>

<h4>Functional Requirements</h4>
<ul>
    <li>Users can <strong>upload a file</strong> from any device</li>
    <li>Users can <strong>download a file</strong> from any device</li>
    <li>Users can <strong>share a file</strong> with other users and view files shared with them</li>
    <li>Users can <strong>automatically sync files</strong> across devices</li>
</ul>

<h4>Non-Functional Requirements</h4>
<ul>
    <li><strong>Highly available</strong> (prioritize availability over consistency)</li>
    <li>Support files as large as <strong>50GB</strong></li>
    <li><strong>Secure and reliable</strong> ‚Äî recover files if lost or corrupted</li>
    <li><strong>Low latency</strong> upload, download, and sync times</li>
</ul>

<h4>Core Entities</h4>
<ul>
    <li><strong>File:</strong> The raw data users upload, download, and share</li>
    <li><strong>FileMetadata:</strong> Name, size, mime type, uploading user ‚Äî stored separately from the file itself</li>
    <li><strong>User:</strong> The system user</li>
</ul>

<h4>API / System Interface</h4>
<ul>
    <li><strong>Upload:</strong> <code>POST /files</code> ‚Äî Request body: { File, FileMetadata }</li>
    <li><strong>Download:</strong> <code>GET /files/{fileId}</code> ‚Üí File & FileMetadata</li>
    <li>Define endpoints early ‚Äî they guide the high-level design</li>
</ul>

<hr>

<h3>High-Level Design</h3>

<h4>1. Upload ‚Äî File from Any Device</h4>
<ul>
    <li>Two things to store: <strong>file contents</strong> (raw bytes) and <strong>file metadata</strong></li>
    <li><strong>Metadata DB:</strong> DynamoDB (NoSQL) ‚Äî loosely structured, main query pattern is fetch files by user. PostgreSQL also works fine.</li>
    <li><strong>File Storage:</strong> S3 (Blob Storage) ‚Äî don't store raw files in the DB</li>
</ul>

<p><strong>Best Approach: Upload Directly to Blob Storage via Presigned URLs</strong></p>
<ul>
    <li>Faster and cheaper than routing files through your backend</li>
    <li><strong>Presigned URLs</strong> = time-limited URLs giving permission to upload to a specific S3 location</li>
    <li>Three-step upload process:</li>
</ul>
<ol>
    <li>Client requests presigned URL from backend: <code>POST /files/presigned-url ‚Üí PresignedUrl</code> (sends FileMetadata, backend saves metadata with status "uploading")</li>
    <li>Client uploads file directly to S3 via <strong>PUT request</strong> to presigned URL</li>
    <li>S3 sends <strong>S3 Notification</strong> to backend ‚Üí updates metadata status to "uploaded"</li>
</ol>

<h4>Upload Architecture</h4>
<pre>
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   POST /files/       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ          ‚îÇ   presigned-url      ‚îÇ  API Gateway /   ‚îÇ        ‚îÇ             ‚îÇ
  ‚îÇ Uploader ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ  Load Balancer   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ File Service‚îÇ
  ‚îÇ          ‚îÇ                      ‚îÇ  - routing       ‚îÇ        ‚îÇ (generates  ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ  - auth          ‚îÇ        ‚îÇ  presigned  ‚îÇ
       ‚îÇ                            ‚îÇ  - rate limiting ‚îÇ        ‚îÇ  URL)       ‚îÇ
       ‚îÇ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                                                              ‚îÇ
       ‚îÇ  Upload file directly                                        ‚îÇ save metadata
       ‚îÇ  via presigned URL                                           ‚îÇ status: "uploading"
       ‚îÇ                                                              ‚Üì
       ‚îÇ                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ    S3    ‚îÇ              ‚îÇ File Metadata‚îÇ
                                    ‚îÇ  (Blob   ‚îÇ ‚îÄ‚îÄ S3 ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ     DB       ‚îÇ
                                    ‚îÇ Storage) ‚îÇ  Notif.     ‚îÇ status ‚Üí     ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ  "uploaded"  ‚îÇ
                                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</pre>

<hr>

<h4>2. Download ‚Äî File from Any Device</h4>
<ul>
    <li><strong>Bad:</strong> Download through File Server (backend bottleneck)</li>
    <li><strong>Good:</strong> Download from Blob Storage (S3 direct)</li>
    <li><strong>Best: Download from CDN</strong></li>
</ul>

<p><strong>CDN Approach:</strong></p>
<ul>
    <li>CDN caches files at 400+ edge locations globally ‚Üí serves from server closest to user</li>
    <li>Much faster than serving from backend or S3 directly</li>
    <li><strong>Security:</strong> Generate presigned CDN URLs (time-limited download permission), same pattern as upload</li>
    <li><strong>Challenge:</strong> CDN is expensive ‚Äî use <strong>cache control headers</strong> to control what gets cached and for how long</li>
    <li>Use <strong>cache invalidation</strong> to remove files from CDN when updated or deleted</li>
    <li>Only frequently accessed files stay cached ‚Üí don't waste money on rarely accessed files</li>
</ul>

<h4>Download Architecture</h4>
<pre>
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  GET /files/{id}   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ            ‚îÇ                    ‚îÇ  API Gateway /   ‚îÇ        ‚îÇ             ‚îÇ
  ‚îÇ Downloader ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ  Load Balancer   ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îÇ File Service‚îÇ
  ‚îÇ            ‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                      ‚îÇ
        ‚îÇ                                                             ‚îÇ lookup metadata
        ‚îÇ  Download file                                              ‚Üì
        ‚îÇ  via presigned                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  CDN URL                                             ‚îÇ File Metadata‚îÇ
        ‚îÇ                                                      ‚îÇ     DB       ‚îÇ
        ‚îÇ                                                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                                                             ‚îÇ
        ‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ return
        ‚îÇ               ‚îÇ          ‚îÇ  cache  ‚îÇ          ‚îÇ             ‚îÇ presigned
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üê ‚îÇ   CDN    ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ    S3    ‚îÇ             ‚îÇ CDN URL
                        ‚îÇ (edge    ‚îÇ  miss   ‚îÇ  (Blob   ‚îÇ             ‚îÇ
                        ‚îÇ servers) ‚îÇ fetch   ‚îÇ Storage) ‚îÇ  ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</pre>
`
            },
            {
                shortcut: "Z",
                title: "Zoom - Video Conferencing",
                tags: ["WebRTC", "SFU", "encoding", "real-time"],
                points: [
                    "Protocol: WebRTC for peer connections, UDP for low latency",
                    "Architecture: SFU (Selective Forwarding Unit) for multi-party",
                    "Encoding: VP8/VP9/H.264, adaptive bitrate per participant",
                    "Quality: Jitter buffer, FEC for packet loss recovery",
                    "Scaling: Regional media servers, route to nearest",
                    "Screen Share: Separate stream, higher resolution priority",
                    "Recording: Server-side capture, async processing"
                ]
            },
            {
                shortcut: "A",
                title: "Ad Click Aggregator",
                tags: ["streaming", "aggregation", "OLAP", "Kafka", "Flink", "real-time"],
                points: [
                    "Collects & aggregates ad click data from multiple sources for real-time analytics",
                    "Functional: Click tracking with 302 redirect, query click metrics at 1-min granularity",
                    "Scale: 10M active ads, 10k clicks/sec peak, 100M clicks/day",
                    "Data Flow: Browser ‚Üí Click Processor ‚Üí Kafka/Kinesis ‚Üí Flink ‚Üí OLAP DB",
                    "302 Redirect: Click hits our server first to track, then redirects to advertiser",
                    "Abuse Prevention: Signed impression IDs as idempotency keys, checked in cache",
                    "Reconciliation: Raw events dumped to S3, Spark batch job validates stream results"
                ],
                walkthrough: `
<h3>Phase 1: Requirements (2-3 mins)</h3>

<h4>Functional Requirements</h4>
<ul>
    <li>Users click an ad ‚Üí redirected to advertiser's website</li>
    <li>Advertisers query ad click metrics over time (1-minute granularity minimum)</li>
</ul>

<h4>Non-Functional Requirements</h4>
<ul>
    <li>10M active ads, peak of 10k clicks/sec, ~100M clicks/day</li>
    <li>Scalable to handle peak load</li>
    <li>Low-latency analytics (sub-second response time)</li>
    <li>Fault-tolerant with accurate data collection</li>
    <li>Near real-time ‚Äî data available ASAP for querying</li>
</ul>

<h4>Data Flow (clarify early)</h4>
<ol>
    <li>User clicks ad on a website</li>
    <li>Click is tracked and stored</li>
    <li>User is redirected to advertiser's website</li>
    <li>Advertiser queries aggregated click metrics</li>
</ol>

<hr>

<h3>Phase 2: High-Level Design (3-4 mins)</h3>
<p><strong>Click path:</strong></p>
<ul>
    <li>User clicks ad placed by Ad Placement Service</li>
    <li>Request goes to <code>/click</code> endpoint ‚Üí Click Processor tracks it</li>
    <li><strong>302 redirect</strong> sends user to advertiser site (not 301 ‚Äî we need every click to hit our server for tracking)</li>
</ul>

<p><strong>Analytics path:</strong></p>
<ul>
    <li>Click Processor writes events to <strong>Kafka/Kinesis</strong></li>
    <li><strong>Flink</strong> reads the stream and aggregates in real-time</li>
    <li>Aggregated data stored in <strong>OLAP DB</strong> (columns: AdId, AdvertiserId as PK, Minute as SK, Click Count)</li>
    <li>Advertisers query OLAP DB via Analytics Service for near-realtime metrics</li>
</ul>

<pre>                    Advertisement:                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     - AdId                           ‚îÇ   Analyst   ‚îÇ
                     - RedirectUrl                    ‚îÇ   Browser   ‚îÇ
                     - ... metadata                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                             ‚Üì
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ Ad Placement  ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Ad DB   ‚îÇ              ‚îÇ  Analytics  ‚îÇ
  ‚îÇ   Service     ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ   Service   ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì                                                  ‚Üë
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          Aggregated Data ‚îÇ
  ‚îÇ               ‚îÇ                            - AdId        ‚îÇ
  ‚îÇ    Browser    ‚îÇ                            - Minute      ‚îÇ
  ‚îÇ               ‚îÇ                            - Click Count ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì                                   ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ    Click      ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ Kinesis  ‚îÇ ‚îÄ‚îÄ‚Üí‚îÇ    Flink    ‚îÇ
  ‚îÇ  Processor    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ             ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚Üì
                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                       ‚îÇ OLAP DB  ‚îÇ
                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</pre>

<hr>

<h3>Phase 3: Deep Dives (6-8 mins)</h3>

<h4>Deep Dive A: Scaling to 10k clicks/sec</h4>
<ul>
    <li><strong>Click Processor:</strong> Scale horizontally with LB distributing load across instances</li>
    <li><strong>Kafka/Kinesis:</strong> Already distributed ‚Äî can handle massive throughput with proper config. Shard by AdId so Flink reads partitions in parallel (independent of each other)</li>
    <li><strong>Flink:</strong> Scale horizontally by adding more tasks/jobs ‚Äî separate jobs reading from each shard</li>
    <li><strong>OLAP DB:</strong> Scale horizontally by adding nodes. Best shard key = <code>advertiserId</code> (advertisers query by their own data)</li>
</ul>

<h4>Deep Dive B: Hot Shards</h4>
<p><strong>Problem:</strong> Popular ads create uneven partition load</p>
<ul>
    <li>Update partition key by appending a random number to AdId (e.g., <code>AdId:0-N</code>)</li>
    <li>This distributes load across multiple shards while still grouping by ad</li>
    <li>Downstream Flink jobs can further aggregate the sub-partitions</li>
</ul>

<h4>Deep Dive C: Preventing Data Loss</h4>
<ol>
    <li><strong>Kafka/Kinesis:</strong> Already distributed, fault-tolerant, highly available ‚Äî replicates across multiple nodes and data centers</li>
    <li><strong>7-day retention:</strong> If stream goes down, data can be replayed</li>
    <li><strong>Flink checkpointing:</strong> Periodically writes processor state to S3 (more useful for large aggregation windows like day/week; our 1-min windows are small)</li>
    <li><strong>Reconciliation:</strong> Dump raw click events to S3 data lake. Run Spark (MapReduce) batch job via cron to compare stream results with raw data ‚Äî fix incorrect records if they don't match</li>
</ol>

<h4>Deep Dive D: Abuse Prevention (duplicate clicks)</h4>
<ol>
    <li>Ad Placement Service generates a unique <strong>impression ID</strong> for each ad shown to a user</li>
    <li>Impression ID is <strong>signed with a secret key</strong> and sent to the browser with the ad</li>
    <li>On click, browser sends the impression ID with click data</li>
    <li>Click Processor checks if impression ID exists in <strong>cache</strong> ‚Äî if found, it's a duplicate ‚Üí ignore. If not, process and add to cache</li>
</ol>

<hr>

<h3>Phase 4: Final Architecture</h3>
<pre>  - generate ad impression id as         Advertisement:                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    idempotency key and store             - AdId                           ‚îÇ   Analyst   ‚îÇ
    in cache                              - RedirectUrl                    ‚îÇ   Browser   ‚îÇ
                                          - ... metadata                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                                                  ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ Ad Placement  ‚îÇ ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Ad DB   ‚îÇ              ‚îÇ     Analytics     ‚îÇ
              ‚îÇ   Service     ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ      Service      ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì                                        Aggregated   ‚Üë
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                  Data:       ‚îÇ
  ‚îÇ          ‚îÇ  ‚îÇ  LB & API ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  - AdId      ‚îÇ
  ‚îÇ Browser  ‚îÇ‚îÄ‚îÄ‚îÇ  Gateway  ‚îÇ    ‚îÇ  Cache   ‚îÇ                  - AdvertiserId (PK)
  ‚îÇ          ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ (dedup)  ‚îÇ                  - Minute (SK)‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚Üì         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   - Click Count‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ                                     ‚îÇ
              ‚îÇ    Click      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Processor    ‚îÇ                                ‚îÇ    OLAP DB    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò
                      ‚Üì           - Both Kinesis and Flink        ‚îÇ   potentially fix
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     partition by AdId with        ‚îÇ   incorrect records
              ‚îÇ   Kinesis     ‚îÇ     celebrity solution for        ‚Üì
              ‚îÇ               ‚îÇ     hot shards              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   - 7 day retention on      ‚îÇReconciliation‚îÇ
                      ‚Üì             Kinesis                 ‚îÇ    Worker    ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ    Flink      ‚îÇ                                    ‚Üë
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚Üì                                     ‚îÇ    Spark     ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ (MapReduce)  ‚îÇ
              ‚îÇ  Raw Click    ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ  Data (S3)    ‚îÇ                                    ‚Üë
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚Üê cron</pre>
<p><strong>Key additions over basic design:</strong></p>
<ul>
    <li>Impression ID idempotency with cache for abuse prevention</li>
    <li>Hot shard handling via partitioned AdId keys</li>
    <li>7-day Kinesis retention for replay</li>
    <li>Reconciliation pipeline (S3 + Spark) for data accuracy</li>
</ul>
`
            }
        ];

        // =====================================================
        // APP LOGIC - NO NEED TO EDIT BELOW
        // =====================================================

        const searchInput = document.getElementById('search');
        const topicsList = document.getElementById('topicsList');
        const indexView = document.getElementById('indexView');
        const counter = document.getElementById('counter');

        let currentMode = 'search';
        let highlightedIndex = -1;
        let visibleTopics = [];

        // Load saved topics from localStorage
        function loadTopics() {
            const saved = localStorage.getItem('teleprompter-topics');
            if (saved) {
                try {
                    topics = JSON.parse(saved);
                } catch (e) {
                    console.log('Using default topics');
                }
            }
        }

        // Save topics to localStorage
        function saveTopics() {
            localStorage.setItem('teleprompter-topics', JSON.stringify(topics));
        }

        // Render all topic cards
        function renderTopics() {
            topicsList.innerHTML = topics.map((topic, i) => `
                <div class="topic-card" data-index="${i}" data-shortcut="${topic.shortcut || ''}">
                    <div class="topic-title">
                        ${topic.shortcut ? `<span class="shortcut-badge">${topic.shortcut}</span>` : ''}
                        ${topic.title}
                    </div>
                    <div class="topic-tags">
                        ${topic.tags.map(tag => `<span class="tag">${tag}</span>`).join('')}
                    </div>
                    ${topic.image ? `
                        <div class="topic-diagram">
                            <img src="${topic.image}" alt="${topic.title} Architecture" onclick="openDiagram('${topic.image}')">
                        </div>
                    ` : ''}
                    <ul class="points-list">
                        ${topic.points.map(point => `<li>${point}</li>`).join('')}
                    </ul>
                    ${topic.walkthrough ? `
                        <div class="walkthrough-content">
                            ${topic.walkthrough}
                        </div>
                    ` : ''}
                </div>
            `).join('');

            // Render index
            indexView.innerHTML = topics.map((topic, i) => `
                <div class="index-item" data-index="${i}">
                    ${topic.shortcut ? `<span class="shortcut-key">${topic.shortcut}</span>` : ''}
                    ${topic.title}
                </div>
            `).join('');

            // Add click handlers to index items
            document.querySelectorAll('.index-item').forEach(item => {
                item.addEventListener('click', () => {
                    const idx = parseInt(item.dataset.index);
                    showSingleTopic(idx);
                });
            });

            updateCounter();
        }

        // Open diagram in fullscreen
        function openDiagram(imageSrc) {
            const overlay = document.getElementById('diagramOverlay');
            const overlayImg = document.getElementById('overlayImage');
            overlayImg.src = imageSrc;
            overlay.classList.add('visible');
        }

        // Close diagram overlay
        function closeDiagram() {
            document.getElementById('diagramOverlay').classList.remove('visible');
        }

        // Close on Escape key
        document.addEventListener('keydown', (e) => {
            if (e.key === 'Escape') {
                closeDiagram();
            }
        });

        // Filter topics based on search
        function filterTopics(query) {
            const cards = document.querySelectorAll('.topic-card');
            visibleTopics = [];
            highlightedIndex = -1;

            if (!query.trim()) {
                cards.forEach(card => card.classList.remove('visible', 'highlighted'));
                updateCounter();
                return;
            }

            const q = query.toLowerCase();

            cards.forEach((card, i) => {
                const topic = topics[i];
                const searchText = [
                    topic.title,
                    ...topic.tags,
                    ...topic.points
                ].join(' ').toLowerCase();

                if (searchText.includes(q)) {
                    card.classList.add('visible');
                    visibleTopics.push(i);
                } else {
                    card.classList.remove('visible');
                }
                card.classList.remove('highlighted');
            });

            // Highlight first match
            if (visibleTopics.length > 0) {
                highlightedIndex = 0;
                document.querySelector(`[data-index="${visibleTopics[0]}"]`).classList.add('highlighted');
            }

            updateCounter();
        }

        // Show single topic (from shortcut or index click)
        function showSingleTopic(idx) {
            const cards = document.querySelectorAll('.topic-card');
            cards.forEach(card => card.classList.remove('visible', 'highlighted'));

            const targetCard = document.querySelector(`.topic-card[data-index="${idx}"]`);
            targetCard.classList.add('visible', 'highlighted');
            targetCard.scrollIntoView({ behavior: 'smooth', block: 'start' });

            visibleTopics = [idx];
            highlightedIndex = 0;

            // Clear search and blur so shortcuts keep working
            searchInput.value = '';
            searchInput.blur();

            // Hide index view if visible
            indexView.classList.remove('visible');

            // Update mode buttons (select search mode visually)
            document.querySelectorAll('.mode-btn').forEach(btn => {
                btn.classList.toggle('active', btn.dataset.mode === 'search');
            });
            currentMode = 'search';

            updateCounter();
        }

        // Update counter
        function updateCounter() {
            const visible = document.querySelectorAll('.topic-card.visible').length;
            if (currentMode === 'all') {
                counter.textContent = `${topics.length} topics`;
            } else if (visible > 0) {
                counter.textContent = `${visible} of ${topics.length} topics`;
            } else if (searchInput.value) {
                counter.textContent = `No matches found`;
            } else {
                counter.textContent = `${topics.length} topics ready ‚Ä¢ Start typing to search`;
            }
        }

        // Set display mode
        function setMode(mode) {
            currentMode = mode;
            document.querySelectorAll('.mode-btn').forEach(btn => {
                btn.classList.toggle('active', btn.dataset.mode === mode);
            });

            const cards = document.querySelectorAll('.topic-card');

            if (mode === 'all') {
                indexView.classList.remove('visible');
                cards.forEach(card => card.classList.add('visible'));
            } else if (mode === 'index') {
                indexView.classList.add('visible');
                cards.forEach(card => card.classList.remove('visible'));
            } else {
                indexView.classList.remove('visible');
                filterTopics(searchInput.value);
            }

            updateCounter();
        }

        // Navigate with arrow keys
        function navigate(direction) {
            if (visibleTopics.length === 0) return;

            const cards = document.querySelectorAll('.topic-card');

            // Remove current highlight
            if (highlightedIndex >= 0) {
                const currentIdx = visibleTopics[highlightedIndex];
                cards[currentIdx].classList.remove('highlighted');
            }

            // Move to next/prev
            highlightedIndex += direction;
            if (highlightedIndex < 0) highlightedIndex = visibleTopics.length - 1;
            if (highlightedIndex >= visibleTopics.length) highlightedIndex = 0;

            // Add new highlight
            const newIdx = visibleTopics[highlightedIndex];
            const newCard = cards[newIdx];
            newCard.classList.add('highlighted');
            newCard.scrollIntoView({ behavior: 'smooth', block: 'center' });
        }

        // Open editor (simple JSON edit for now)
        function openEditor() {
            const json = JSON.stringify(topics, null, 2);
            const newJson = prompt('Edit topics JSON (or paste new topics):', json);
            if (newJson) {
                try {
                    topics = JSON.parse(newJson);
                    saveTopics();
                    renderTopics();
                    alert('Topics updated!');
                } catch (e) {
                    alert('Invalid JSON format');
                }
            }
        }

        // Event Listeners
        searchInput.addEventListener('input', (e) => {
            if (currentMode !== 'search') setMode('search');
            filterTopics(e.target.value);
        });

        searchInput.addEventListener('keydown', (e) => {
            if (e.key === 'ArrowDown') {
                e.preventDefault();
                navigate(1);
            } else if (e.key === 'ArrowUp') {
                e.preventDefault();
                navigate(-1);
            } else if (e.key === 'Escape') {
                searchInput.value = '';
                filterTopics('');
                setMode('search');
            } else if (e.key === 'Tab') {
                e.preventDefault();
                setMode(currentMode === 'all' ? 'search' : 'all');
            }
        });

        document.querySelectorAll('.mode-btn').forEach(btn => {
            btn.addEventListener('click', () => setMode(btn.dataset.mode));
        });

        // Keyboard shortcuts for A-Z (when not typing in search)
        document.addEventListener('keydown', (e) => {
            // Only trigger if not focused on search input
            if (document.activeElement === searchInput) return;

            const key = e.key.toUpperCase();
            if (key.length === 1 && key >= 'A' && key <= 'Z') {
                const topicIndex = topics.findIndex(t => t.shortcut === key);
                if (topicIndex !== -1) {
                    e.preventDefault();
                    showSingleTopic(topicIndex);
                }
            }
        });

        // Initialize
        loadTopics();
        renderTopics();
        searchInput.focus();
    </script>
</body>

</html>